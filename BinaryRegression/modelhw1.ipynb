{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 221\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1-score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 221\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 208\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m X \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokenized_sentences)):\n\u001b[1;32m--> 208\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend([tf_idf[i][word] \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokenized_sentences[i] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m vocab])\n\u001b[0;32m    209\u001b[0m y \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mham\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[0;32m    211\u001b[0m model\u001b[38;5;241m.\u001b[39mSGD_training(X, y)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load and preprocess the dataset\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load the dataset and split into sentences and labels.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            labels.append(parts[0])\n",
    "            sentences.append(parts[1])\n",
    "    \n",
    "    return sentences, labels\n",
    "data = (load_data(\"SMSSpamCollection\"))\n",
    "\n",
    "#Loaded data, we have an array of each sentence, and then array of all of the labels in a row \n",
    "# print(data[1])\n",
    "# print(data[0])\n",
    "\n",
    "\n",
    "# Preprocessing: Remove punctuation, URLs, numbers, and convert to lowercase\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans text by removing punctuation, URLs, and numbers.\"\"\"\n",
    "    cleaned_text = \"\".join([char.lower() for char in text if char.isalpha() or char.isspace()])\n",
    "    return cleaned_text\n",
    "\n",
    "# Step 2: Calculate Term Frequency (TF)\n",
    "def compute_tf(doc):\n",
    "    tf = Counter(doc)\n",
    "    doc_length = len(doc)\n",
    "    for word in tf:\n",
    "        tf[word] /= doc_length  # Normalize by document length\n",
    "    return tf\n",
    "\n",
    "# Step 3: Calculate Inverse Document Frequency (IDF)\n",
    "def compute_idf(corpus):\n",
    "    idf = {}\n",
    "    total_docs = len(corpus)\n",
    "    word_doc_count = Counter()\n",
    "    \n",
    "    # Count how many documents contain each word\n",
    "    for doc in corpus:\n",
    "        unique_words = set(doc)  # Remove duplicates\n",
    "        for word in unique_words:\n",
    "            word_doc_count[word] += 1\n",
    "    \n",
    "    # Calculate IDF for each word\n",
    "    for word, doc_count in word_doc_count.items():\n",
    "        idf[word] = math.log(total_docs / (1 + doc_count))  # Add 1 to avoid division by 0\n",
    "    return idf\n",
    "\n",
    "# Step 4: Compute TF-IDF\n",
    "def compute_tfidf(corpus):\n",
    "    tfidf = []\n",
    "    idf = compute_idf(corpus)  # Compute IDF first\n",
    "    \n",
    "    for doc in corpus:\n",
    "        tf = compute_tf(doc)  # Compute TF for this document\n",
    "        tfidf_doc = {}\n",
    "        \n",
    "        for word, tf_value in tf.items():\n",
    "            tfidf_doc[word] = tf_value * idf[word]  # Compute TF-IDF\n",
    "        tfidf.append(tfidf_doc)\n",
    "    \n",
    "    return tfidf\n",
    "\n",
    "# Tokenization and feature extraction (TF-IDF)\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenizes text and removes stopwords (hardcoded).\"\"\"\n",
    "    stopwords = {\"the\", \"is\", \"in\", \"and\", \"to\", \"of\", \"a\", \"i\", \"it\"}  # Minimal stopword set\n",
    "    tokens = [word for word in text.split() if word not in stopwords and len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cleaned = preprocess_text(data[0][0])\n",
    "# print(cleaned)\n",
    "# tokenized = tokenize(cleaned)\n",
    "# print(tokenized)\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.001, lambda_reg=0.1, num_epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = []\n",
    "        self.bias = 0\n",
    "\n",
    "\n",
    "    def calculate_gradient(self, X, y, pred):\n",
    "        dw = [0] * len(X[0])\n",
    "        db = 0\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            error = pred[i] - y[i]\n",
    "            for j in range(len(X[0])):\n",
    "                dw[j] += error * X[i][j]\n",
    "            db += error\n",
    "\n",
    "        for i in range(len(X[0])):\n",
    "            dw[i] = 1/len(y) * dw[i] + 2 * self.lambda_reg * self.weights[i]\n",
    "        db = 1/len(y) * db\n",
    "\n",
    "        return dw, db\n",
    "    \n",
    "\n",
    "    def obj_func(self, X, y, pred):\n",
    "        loss = 0\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            epsilon = 1e-15  # Small value to avoid log(0)\n",
    "            pred[i] = max(epsilon, min(1 - epsilon, pred[i]))  # Clamp the value of pred\n",
    "            loss += y[i] * math.log(pred[i]) + (1 - y[i]) * math.log(1 - pred[i])\n",
    "\n",
    "        loss = (-1/len(y)) * loss\n",
    "        reg = 0\n",
    "        for weight in self.weights:\n",
    "            reg += weight ** 2\n",
    "\n",
    "        return loss + self.lambda_reg * reg\n",
    "\n",
    "\n",
    "    def SGD_training(self, X, y):\n",
    "        self.weights = [0] * len(X[0])\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] = random.uniform(-0.1, 0.1)\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            loss = 0\n",
    "            for i in range(len(y)):\n",
    "                pred = 0\n",
    "                for j in range(len(X[0])):\n",
    "                    pred += X[i][j] * self.weights[j]\n",
    "                pred += self.bias\n",
    "                pred = 1 / (1 + math.exp(-pred))\n",
    "                dw, db = self.calculate_gradient([X[i]], [y[i]], [pred])\n",
    "                for j in range(len(X[0])):\n",
    "                    self.weights[j] -= dw[j] * self.learning_rate\n",
    "                self.bias -= db * self.learning_rate\n",
    "                loss += self.obj_func([X[i]], [y[i]], [pred])\n",
    "            print(\"Loss: \", loss/len(y))\n",
    "        return\n",
    "    \n",
    "\n",
    "    def MbGD_training(self, X, y, pred):\n",
    "        return\n",
    "    \n",
    "\n",
    "    def Evaluate(self):\n",
    "        return\n",
    "    \n",
    "\n",
    "    def train(self, X, y):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            pred = [0] * len(y)\n",
    "            for i in range(len(y)):\n",
    "                pred[i] = 1 / (1 + math.exp(-(X[i][0] * self.weight + self.bias)))\n",
    "\n",
    "            self.SGD(X, y, pred)\n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Add prediction function to LogisticRegression class\"\"\"\n",
    "        predictions = []\n",
    "        for i in range(len(X)):\n",
    "            pred = 0\n",
    "            for j in range(len(X[0])):\n",
    "                pred += X[i][j] * self.weights[j]\n",
    "            pred += self.bias\n",
    "            pred = 1 / (1 + math.exp(-pred))\n",
    "            predictions.append(1 if pred >= 0.5 else 0)\n",
    "        return predictions\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"Computes accuracy, precision, recall, and F1-score.\"\"\"\n",
    "    tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n",
    "    tn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 0)\n",
    "    fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n",
    "    fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n",
    "    \n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to load data, train the model, and print results.\"\"\"\n",
    "    file_path = \"SMSSpamCollection\"  # Update with actual dataset path\n",
    "    sentences, labels = load_data(file_path)\n",
    "    preprocessed_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "    tokenized_sentences = [tokenize(sentence) for sentence in preprocessed_sentences]\n",
    "    tf_idf = compute_tfidf(tokenized_sentences)\n",
    "    vocab = set()\n",
    "    for sentence in tokenized_sentences:\n",
    "        vocab.update(sentence)\n",
    "    \n",
    "    model = LogisticRegression(num_epochs=7)\n",
    "    # TODO: Add TF-IDF feature extraction\n",
    "    X = []\n",
    "    for i in range(len(tokenized_sentences)):\n",
    "        X.append([tf_idf[i][word] if word in tokenized_sentences[i] else 0 for word in vocab])\n",
    "    y = [0 if label==\"ham\" else 1 for label in labels]\n",
    "\n",
    "    model.SGD_training(X, y)\n",
    "    \n",
    "    predictions = model.predict(X)\n",
    "    accuracy, precision, recall, f1 = evaluate(y, predictions)\n",
    "    \n",
    "    print(\"Final Evaluation:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from SMSSpamCollection...\n",
      "Total number of samples loaded: 5574\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 366\u001b[0m\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1-score:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 366\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 348\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    345\u001b[0m y_test \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspam\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m test_labels]\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# Find best lambda using cross-validation\u001b[39;00m\n\u001b[1;32m--> 348\u001b[0m best_lambda \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest lambda from cross-validation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_lambda\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# Train final model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 312\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(X, y, k_folds, lambda_values)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m# Train and evaluate model\u001b[39;00m\n\u001b[0;32m    311\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression(lambda_reg\u001b[38;5;241m=\u001b[39mlambda_val)\n\u001b[1;32m--> 312\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m    314\u001b[0m accuracy, _, _, _ \u001b[38;5;241m=\u001b[39m evaluate(y_val, y_pred)\n",
      "Cell \u001b[1;32mIn[8], line 212\u001b[0m, in \u001b[0;36mLogisticRegression.train_sgd\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    209\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[0;32m    213\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(z)\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# Compute weighted error\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 212\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    209\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(X[i][j] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[j] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights))) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[0;32m    213\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(z)\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# Compute weighted error\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "def load_and_split_data(file_path, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Loads text data from a file and splits it into training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    print(f\"Reading data from {file_path}...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"Total number of samples loaded: {len(lines)}\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    # Calculate split points\n",
    "    train_end = int(len(lines) * train_ratio)\n",
    "    val_end = int(len(lines) * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Split data\n",
    "    train_data = lines[:train_end]\n",
    "    val_data = lines[train_end:val_end]\n",
    "    test_data = lines[val_end:]\n",
    "    \n",
    "    # Print data distribution\n",
    "    print(\"\\nData Distribution:\")\n",
    "    print(f\"Training samples: {len(train_data)}\")\n",
    "    print(f\"Validation samples: {len(val_data)}\")\n",
    "    print(f\"Test samples: {len(test_data)}\")\n",
    "    \n",
    "    def split_data(data):\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in data:\n",
    "            if '\\t' in line:  # Check if line contains tab separator\n",
    "                label, text = line.strip().split('\\t', 1)\n",
    "                labels.append(label)\n",
    "                sentences.append(text)\n",
    "        return sentences, labels\n",
    "    \n",
    "    return split_data(train_data) + split_data(val_data) + split_data(test_data)\n",
    "\n",
    "# [Previous preprocess_text and tokenize functions remain the same]\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, lambda_reg=0.1, num_epochs=100, \n",
    "                 early_stop_threshold=1e-4, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "        self.training_history = []\n",
    "    \n",
    "    def train_sgd(self, X, y):\n",
    "        \"\"\"Train using Stochastic Gradient Descent with detailed logging.\"\"\"\n",
    "        if not self.weights:\n",
    "            self.weights = [random.uniform(-0.1, 0.1) for _ in range(len(X[0]))]\n",
    "        \n",
    "        class_weights = self.compute_class_weights(y)\n",
    "        n_samples = len(y)\n",
    "        prev_loss = float('inf')\n",
    "        \n",
    "        print(\"\\nTraining with SGD:\")\n",
    "        print(\"Epoch\\tLoss\\t\\tΔLoss\")\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            indices = list(range(n_samples))\n",
    "            random.shuffle(indices)\n",
    "            \n",
    "            total_loss = 0\n",
    "            for i in indices:\n",
    "                # Forward pass\n",
    "                z = sum(X[i][j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "                pred = self.sigmoid(z)\n",
    "                \n",
    "                # Compute weighted error\n",
    "                weight = class_weights[y[i]]\n",
    "                error = weight * (pred - y[i])\n",
    "                \n",
    "                # Update weights and bias\n",
    "                for j in range(len(self.weights)):\n",
    "                    gradient = error * X[i][j] + 2 * self.lambda_reg * self.weights[j]\n",
    "                    self.weights[j] -= self.learning_rate * gradient\n",
    "                self.bias -= self.learning_rate * error\n",
    "                \n",
    "                total_loss += self.compute_loss(X, y, [i])\n",
    "            \n",
    "            avg_loss = total_loss / n_samples\n",
    "            loss_change = prev_loss - avg_loss\n",
    "            self.training_history.append(avg_loss)\n",
    "            \n",
    "            if epoch % 5 == 0:  # Print every 5 epochs\n",
    "                print(f\"{epoch}\\t{avg_loss:.6f}\\t{loss_change:.6f}\")\n",
    "            \n",
    "            if epoch > 0 and abs(loss_change) < self.early_stop_threshold:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "            prev_loss = avg_loss\n",
    "\n",
    "def main():\n",
    "    # Load and split data\n",
    "    print(\"Step 1: Loading and splitting data...\")\n",
    "    train_sentences, train_labels, val_sentences, val_labels, test_sentences, test_labels = \\\n",
    "        load_and_split_data(\"SMSSpamCollection\")\n",
    "    \n",
    "    # Print label distribution\n",
    "    for dataset_name, labels in [(\"Training\", train_labels), \n",
    "                               (\"Validation\", val_labels), \n",
    "                               (\"Test\", test_labels)]:\n",
    "        spam_count = sum(1 for label in labels if label == \"spam\")\n",
    "        ham_count = len(labels) - spam_count\n",
    "        print(f\"\\n{dataset_name} set distribution:\")\n",
    "        print(f\"Spam: {spam_count} ({spam_count/len(labels)*100:.1f}%)\")\n",
    "        print(f\"Ham: {ham_count} ({ham_count/len(labels)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nStep 2: Preprocessing and tokenizing text...\")\n",
    "    train_processed = [tokenize(preprocess_text(text)) for text in train_sentences]\n",
    "    val_processed = [tokenize(preprocess_text(text)) for text in val_sentences]\n",
    "    test_processed = [tokenize(preprocess_text(text)) for text in test_sentences]\n",
    "    \n",
    "    print(\"\\nStep 3: Extracting TF-IDF features...\")\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(train_processed)\n",
    "    print(f\"Vocabulary size: {len(vectorizer.vocabulary)}\")\n",
    "    \n",
    "    X_train = vectorizer.transform(train_processed)\n",
    "    X_val = vectorizer.transform(val_processed)\n",
    "    X_test = vectorizer.transform(test_processed)\n",
    "    \n",
    "    # Convert labels\n",
    "    y_train = [1 if label == \"spam\" else 0 for label in train_labels]\n",
    "    y_val = [1 if label == \"spam\" else 0 for label in val_labels]\n",
    "    y_test = [1 if label == \"spam\" else 0 for label in test_labels]\n",
    "    \n",
    "    print(\"\\nStep 4: Cross-validation for lambda selection...\")\n",
    "    lambda_values = [0.001, 0.01, 0.1, 1.0]\n",
    "    best_lambda = cross_validate(X_train, y_train, k_folds=5, lambda_values=lambda_values)\n",
    "    print(f\"Best lambda from cross-validation: {best_lambda}\")\n",
    "    \n",
    "    print(\"\\nStep 5: Training final model...\")\n",
    "    model = LogisticRegression(lambda_reg=best_lambda, num_epochs=100)\n",
    "    model.train_sgd(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nStep 6: Evaluating on test set...\")\n",
    "    test_pred = model.predict(X_test)\n",
    "    accuracy, precision, recall, f1 = evaluate(y_test, test_pred)\n",
    "    \n",
    "    print(\"\\nFinal Test Results:\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from SMSSpamCollection...\n",
      "Total number of samples loaded: 5574\n",
      "\n",
      "Label Distribution:\n",
      "ham: 4827 (86.60%)\n",
      "spam: 747 (13.40%)\n",
      "\n",
      "Preprocessing and tokenizing texts...\n",
      "\n",
      "Step 4: Training with SGD...\n",
      "Class weights: {0: 0.5758783584292885, 1: 3.794747081712062}\n",
      "\n",
      "Training with SGD:\n",
      "Epoch\tLoss\t\tΔLoss\n",
      "0\t0.565894\tinf\n",
      "5\t0.548030\t0.004362\n",
      "10\t0.551706\t0.003164\n",
      "15\t0.549920\t0.003600\n",
      "20\t0.551242\t0.002120\n",
      "25\t0.552730\t-0.001959\n",
      "30\t0.554134\t-0.004919\n",
      "\n",
      "Step 5: Evaluating SGD model...\n",
      "\n",
      "SGD Validation Results:\n",
      "Accuracy:  0.9187\n",
      "Precision: 0.6524\n",
      "Recall:    0.9068\n",
      "F1-score:  0.7589\n",
      "\n",
      "SGD Test Results:\n",
      "Accuracy:  0.9283\n",
      "Precision: 0.6821\n",
      "Recall:    0.8957\n",
      "F1-score:  0.7744\n",
      "\n",
      "Step 4: Training with MINIBATCH...\n",
      "Class weights: {0: 0.5758783584292885, 1: 3.794747081712062}\n",
      "\n",
      "Training with MINIBATCH:\n",
      "Epoch\tLoss\t\tΔLoss\n",
      "0\t0.826326\tinf\n",
      "5\t0.604667\t0.011946\n",
      "10\t0.589657\t0.001560\n",
      "15\t0.588511\t-0.005011\n",
      "20\t0.580150\t0.010970\n",
      "25\t0.592079\t-0.009504\n",
      "30\t0.588796\t-0.002602\n",
      "\n",
      "Step 5: Evaluating MINIBATCH model...\n",
      "\n",
      "MINIBATCH Validation Results:\n",
      "Accuracy:  0.9414\n",
      "Precision: 0.7518\n",
      "Recall:    0.8729\n",
      "F1-score:  0.8078\n",
      "\n",
      "MINIBATCH Test Results:\n",
      "Accuracy:  0.9498\n",
      "Precision: 0.7744\n",
      "Recall:    0.8957\n",
      "F1-score:  0.8306\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "def load_and_split_data(file_path, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Loads text data from a file and splits it into training, validation, and test sets.\n",
    "    Args:\n",
    "        file_path (str): Path to the data file\n",
    "        train_ratio (float): Proportion of data for training\n",
    "        val_ratio (float): Proportion of data for validation\n",
    "    Returns:\n",
    "        tuple: Contains train, validation, and test data and labels\n",
    "    \"\"\"\n",
    "    print(f\"Reading data from {file_path}...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"Total number of samples loaded: {len(lines)}\")\n",
    "    \n",
    "    # Count label distribution\n",
    "    label_counts = Counter(line.split('\\t')[0] for line in lines)\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"{label}: {count} ({count/len(lines)*100:.2f}%)\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    # Calculate split points\n",
    "    train_end = int(len(lines) * train_ratio)\n",
    "    val_end = int(len(lines) * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Split data\n",
    "    train_data = lines[:train_end]\n",
    "    val_data = lines[train_end:val_end]\n",
    "    test_data = lines[val_end:]\n",
    "    \n",
    "    def split_data(data):\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in data:\n",
    "            if '\\t' in line:\n",
    "                label, text = line.strip().split('\\t', 1)\n",
    "                # Convert 'spam' to 1 and 'ham' to 0\n",
    "                label_int = 1 if label.lower() == 'spam' else 0\n",
    "                labels.append(label_int)\n",
    "                sentences.append(text)\n",
    "        return sentences, labels\n",
    "    \n",
    "    return split_data(train_data) + split_data(val_data) + split_data(test_data)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing special characters and converting to lowercase.\n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = ' '.join(word for word in text.split() \n",
    "                   if not word.startswith(('http:', 'https:', 'www.')))\n",
    "    \n",
    "    # Keep only letters and spaces\n",
    "    cleaned_text = \"\"\n",
    "    for char in text:\n",
    "        if char.isalpha() or char.isspace():\n",
    "            cleaned_text += char\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Converts text into tokens, removes stopwords, and applies stemming.\n",
    "    Args:\n",
    "        text (str): Input text to tokenize\n",
    "    Returns:\n",
    "        list: List of processed tokens\n",
    "    \"\"\"\n",
    "    stopwords = {\n",
    "        \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\", \n",
    "        \"has\", \"he\", \"in\", \"is\", \"it\", \"its\", \"of\", \"on\", \"that\", \"the\", \n",
    "        \"to\", \"was\", \"were\", \"will\", \"with\", \"the\", \"this\", \"but\", \"they\",\n",
    "        \"have\", \"had\", \"what\", \"when\", \"where\", \"who\", \"which\", \"why\", \"how\"\n",
    "    }\n",
    "    \n",
    "    def simple_stem(word):\n",
    "        \"\"\"Simple word stemming rules\"\"\"\n",
    "        if len(word) < 4:\n",
    "            return word\n",
    "        if word.endswith('ing'):\n",
    "            return word[:-3]\n",
    "        elif word.endswith('ed'):\n",
    "            return word[:-2]\n",
    "        elif word.endswith('s'):\n",
    "            return word[:-1]\n",
    "        return word\n",
    "    \n",
    "    words = text.split()\n",
    "    return [simple_stem(word) for word in words \n",
    "            if word not in stopwords and len(word) > 1]\n",
    "\n",
    "class TfidfVectorizer:\n",
    "    \"\"\"\n",
    "    Converts text documents to TF-IDF feature vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocabulary = set()\n",
    "        self.idf = {}\n",
    "        self.vocab_index = {}\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"\n",
    "        Builds vocabulary and computes IDF scores from documents\n",
    "        Args:\n",
    "            documents (list): List of tokenized documents\n",
    "        \"\"\"\n",
    "        # Build vocabulary\n",
    "        for doc in documents:\n",
    "            self.vocabulary.update(doc)\n",
    "        \n",
    "        self.vocab_index = {word: idx for idx, word in enumerate(sorted(self.vocabulary))}\n",
    "        \n",
    "        # Compute document frequencies\n",
    "        doc_freq = Counter()\n",
    "        for doc in documents:\n",
    "            doc_words = set(doc)\n",
    "            for word in doc_words:\n",
    "                doc_freq[word] += 1\n",
    "        \n",
    "        # Calculate IDF scores\n",
    "        num_docs = len(documents)\n",
    "        self.idf = {word: math.log((num_docs + 1)/(doc_freq[word] + 1)) + 1 \n",
    "                   for word in self.vocabulary}\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"\n",
    "        Transforms documents into TF-IDF feature vectors\n",
    "        Args:\n",
    "            documents (list): List of tokenized documents\n",
    "        Returns:\n",
    "            list: List of TF-IDF feature vectors\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        for doc in documents:\n",
    "            tf = Counter(doc)\n",
    "            doc_len = len(doc) if len(doc) > 0 else 1\n",
    "            \n",
    "            features = [0.0] * len(self.vocabulary)\n",
    "            for word in set(doc):\n",
    "                if word in self.vocab_index:\n",
    "                    idx = self.vocab_index[word]\n",
    "                    tf_val = tf[word] / doc_len\n",
    "                    features[idx] = tf_val * self.idf.get(word, 0)\n",
    "            \n",
    "            X.append(features)\n",
    "        return X\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, lambda_reg=0.1, num_epochs=100, \n",
    "                 early_stop_threshold=1e-4, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "        self.training_history = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\"\"\"\n",
    "        # Clip z to prevent overflow\n",
    "        z = min(max(z, -100), 100)\n",
    "        return 1 / (1 + math.exp(-z))\n",
    "    \n",
    "    def compute_loss(self, X, y, indices):\n",
    "        \"\"\"Compute binary cross-entropy loss with L2 regularization\"\"\"\n",
    "        loss = 0\n",
    "        for i in indices:\n",
    "            z = sum(X[i][j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "            pred = self.sigmoid(z)\n",
    "            # Add small epsilon to avoid log(0)\n",
    "            loss -= (y[i] * math.log(pred + 1e-15) + (1 - y[i]) * math.log(1 - pred + 1e-15))\n",
    "        \n",
    "        # Add L2 regularization term\n",
    "        l2_term = self.lambda_reg * sum(w * w for w in self.weights)\n",
    "        return (loss / len(indices)) + l2_term\n",
    "    \n",
    "    def compute_class_weights(self, y):\n",
    "        \"\"\"\n",
    "        Compute class weights to handle class imbalance\n",
    "        Args:\n",
    "            y (list): Labels\n",
    "        Returns:\n",
    "            dict: Class weights\n",
    "        \"\"\"\n",
    "        counts = Counter(y)\n",
    "        total = len(y)\n",
    "        weights = {\n",
    "            0: total / (2 * counts[0]) if counts[0] > 0 else 1,\n",
    "            1: total / (2 * counts[1]) if counts[1] > 0 else 1\n",
    "        }\n",
    "        print(f\"Class weights: {weights}\")\n",
    "        return weights\n",
    "    \n",
    "    def train(self, X, y, method='sgd'):\n",
    "        \"\"\"\n",
    "        Train the model using either SGD or mini-batch gradient descent\n",
    "        Args:\n",
    "            X (list): Feature vectors\n",
    "            y (list): Labels\n",
    "            method (str): 'sgd' or 'minibatch'\n",
    "        \"\"\"\n",
    "        if not self.weights:\n",
    "            # Initialize weights with small random values\n",
    "            self.weights = [random.uniform(-0.1, 0.1) for _ in range(len(X[0]))]\n",
    "        \n",
    "        # Compute class weights for balanced training\n",
    "        class_weights = self.compute_class_weights(y)\n",
    "        \n",
    "        print(f\"\\nTraining with {method.upper()}:\")\n",
    "        print(\"Epoch\\tLoss\\t\\tΔLoss\")\n",
    "        \n",
    "        n_samples = len(y)\n",
    "        prev_loss = float('inf')\n",
    "        no_improvement_count = 0\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            indices = list(range(n_samples))\n",
    "            random.shuffle(indices)\n",
    "            \n",
    "            if method == 'sgd':\n",
    "                batch_indices = [[i] for i in indices]\n",
    "            else:\n",
    "                batch_indices = [indices[i:i + self.batch_size] \n",
    "                               for i in range(0, len(indices), self.batch_size)]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            for batch in batch_indices:\n",
    "                weight_gradients = [0] * len(self.weights)\n",
    "                bias_gradient = 0\n",
    "                \n",
    "                for i in batch:\n",
    "                    # Forward pass\n",
    "                    z = sum(X[i][j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "                    pred = self.sigmoid(z)\n",
    "                    \n",
    "                    # Apply class weights to error\n",
    "                    sample_weight = class_weights[y[i]]\n",
    "                    error = sample_weight * (pred - y[i])\n",
    "                    \n",
    "                    # Accumulate gradients\n",
    "                    for j in range(len(self.weights)):\n",
    "                        weight_gradients[j] += error * X[i][j]\n",
    "                    bias_gradient += error\n",
    "                \n",
    "                # Apply updates with regularization\n",
    "                batch_size = len(batch)\n",
    "                for j in range(len(self.weights)):\n",
    "                    reg_gradient = 2 * self.lambda_reg * self.weights[j]\n",
    "                    self.weights[j] -= self.learning_rate * (weight_gradients[j]/batch_size + reg_gradient)\n",
    "                self.bias -= self.learning_rate * (bias_gradient/batch_size)\n",
    "                \n",
    "                epoch_loss += self.compute_loss(X, y, batch)\n",
    "            \n",
    "            avg_loss = epoch_loss / len(batch_indices)\n",
    "            loss_change = prev_loss - avg_loss\n",
    "            self.training_history.append(avg_loss)\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"{epoch}\\t{avg_loss:.6f}\\t{loss_change:.6f}\")\n",
    "            \n",
    "            # Early stopping with patience\n",
    "            if abs(loss_change) < self.early_stop_threshold:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= 3:  # Wait for 3 epochs of no improvement\n",
    "                    print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            else:\n",
    "                no_improvement_count = 0\n",
    "            \n",
    "            prev_loss = avg_loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            z = sum(x[j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "            prob = self.sigmoid(z)\n",
    "            predictions.append(1 if prob >= 0.5 else 0)\n",
    "        return predictions\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics\n",
    "    Returns:\n",
    "        tuple: (accuracy, precision, recall, f1)\n",
    "    \"\"\"\n",
    "    tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n",
    "    tn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 0)\n",
    "    fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n",
    "    fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n",
    "    \n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def perform_cross_validation(X, y, lambda_values, k=5):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation to find the best regularization parameter\n",
    "    Args:\n",
    "        X (list): Feature vectors\n",
    "        y (list): Labels\n",
    "        lambda_values (list): List of lambda values to try\n",
    "        k (int): Number of folds\n",
    "    Returns:\n",
    "        float: Best lambda value\n",
    "    \"\"\"\n",
    "    print(\"\\nPerforming cross-validation...\")\n",
    "    n_samples = len(y)\n",
    "    fold_size = n_samples // k\n",
    "    best_lambda = None\n",
    "    best_score = -float('inf')\n",
    "    \n",
    "    for lambda_val in lambda_values:\n",
    "        print(f\"\\nTrying lambda = {lambda_val}\")\n",
    "        scores = []\n",
    "        \n",
    "        for fold in range(k):\n",
    "            # Split data into training and validation\n",
    "            val_start = fold * fold_size\n",
    "            val_end = val_start + fold_size\n",
    "            \n",
    "            X_val = X[val_start:val_end]\n",
    "            y_val = y[val_start:val_end]\n",
    "            X_train = X[:val_start] + X[val_end:]\n",
    "            y_train = y[:val_start] + y[val_end:]\n",
    "            \n",
    "            # Train model\n",
    "            model = LogisticRegression(lambda_reg=lambda_val, num_epochs=50)\n",
    "            model.train(X_train, y_train, method='minibatch')\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_val)\n",
    "            accuracy, _, _, f1 = evaluate(y_val, y_pred)\n",
    "            scores.append(f1)\n",
    "        \n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        print(f\"Average F1-score: {avg_score:.4f}\")\n",
    "        \n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_lambda = lambda_val\n",
    "    \n",
    "    print(f\"\\nBest lambda value: {best_lambda} (F1-score: {best_score:.4f})\")\n",
    "    return best_lambda\n",
    "\n",
    "def main():\n",
    "    # Example usage with a sample input file\n",
    "    # Assuming input file format: label\\ttext\n",
    "    file_path = \"SMSSpamCollection\"  # Replace with your data file path\n",
    "    \n",
    "    # Step 1: Load and split data\n",
    "    train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = load_and_split_data(file_path)\n",
    "    \n",
    "    # Step 2: Preprocess and tokenize texts\n",
    "    print(\"\\nPreprocessing and tokenizing texts...\")\n",
    "    train_tokens = [tokenize(preprocess_text(text)) for text in train_texts]\n",
    "    val_tokens = [tokenize(preprocess_text(text)) for text in val_texts]\n",
    "    test_tokens = [tokenize(preprocess_text(text)) for text in test_texts]\n",
    "    \n",
    "    # Step 3: Create TF-IDF features\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(train_tokens)\n",
    "    \n",
    "    X_train = vectorizer.transform(train_tokens)\n",
    "    X_val = vectorizer.transform(val_tokens)\n",
    "    X_test = vectorizer.transform(test_tokens)\n",
    "    \n",
    "    # Step 4: Train and evaluate with both SGD and mini-batch\n",
    "    for method in ['sgd', 'minibatch']:\n",
    "        print(f\"\\nStep 4: Training with {method.upper()}...\")\n",
    "        model = LogisticRegression(\n",
    "            learning_rate=0.1,  # Increased learning rate\n",
    "            lambda_reg=0.01,    # Reduced regularization\n",
    "            num_epochs=35,\n",
    "            early_stop_threshold=1e-4,\n",
    "            batch_size=32\n",
    "        )\n",
    "        model.train(X_train, train_labels, method=method)\n",
    "        \n",
    "        # Evaluate on both validation and test sets\n",
    "        print(f\"\\nStep 5: Evaluating {method.upper()} model...\")\n",
    "        \n",
    "        # Validation set evaluation\n",
    "        val_pred = model.predict(X_val)\n",
    "        val_accuracy, val_precision, val_recall, val_f1 = evaluate(val_labels, val_pred)\n",
    "        print(f\"\\n{method.upper()} Validation Results:\")\n",
    "        print(f\"Accuracy:  {val_accuracy:.4f}\")\n",
    "        print(f\"Precision: {val_precision:.4f}\")\n",
    "        print(f\"Recall:    {val_recall:.4f}\")\n",
    "        print(f\"F1-score:  {val_f1:.4f}\")\n",
    "        \n",
    "        # Test set evaluation\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_accuracy, test_precision, test_recall, test_f1 = evaluate(test_labels, test_pred)\n",
    "        print(f\"\\n{method.upper()} Test Results:\")\n",
    "        print(f\"Accuracy:  {test_accuracy:.4f}\")\n",
    "        print(f\"Precision: {test_precision:.4f}\")\n",
    "        print(f\"Recall:    {test_recall:.4f}\")\n",
    "        print(f\"F1-score:  {test_f1:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
