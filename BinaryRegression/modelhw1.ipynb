{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from SMSSpamCollection...\n",
      "Total number of samples loaded: 5574\n",
      "\n",
      "Label Distribution:\n",
      "ham: 4827 (86.60%)\n",
      "spam: 747 (13.40%)\n",
      "\n",
      "Preprocessing and tokenizing texts...\n",
      "\n",
      "Step 4: Training with SGD...\n",
      "Class weights: {0: 0.5758783584292885, 1: 3.794747081712062}\n",
      "\n",
      "Training with SGD:\n",
      "Epoch\tLoss\t\tΔLoss\n",
      "0\t0.565894\tinf\n",
      "5\t0.548030\t0.004362\n",
      "10\t0.551706\t0.003164\n",
      "15\t0.549920\t0.003600\n",
      "20\t0.551242\t0.002120\n",
      "25\t0.552730\t-0.001959\n",
      "30\t0.554134\t-0.004919\n",
      "\n",
      "Step 5: Evaluating SGD model...\n",
      "\n",
      "SGD Validation Results:\n",
      "Accuracy:  0.9187\n",
      "Precision: 0.6524\n",
      "Recall:    0.9068\n",
      "F1-score:  0.7589\n",
      "\n",
      "SGD Test Results:\n",
      "Accuracy:  0.9283\n",
      "Precision: 0.6821\n",
      "Recall:    0.8957\n",
      "F1-score:  0.7744\n",
      "\n",
      "Step 4: Training with MINIBATCH...\n",
      "Class weights: {0: 0.5758783584292885, 1: 3.794747081712062}\n",
      "\n",
      "Training with MINIBATCH:\n",
      "Epoch\tLoss\t\tΔLoss\n",
      "0\t0.826326\tinf\n",
      "5\t0.604667\t0.011946\n",
      "10\t0.589657\t0.001560\n",
      "15\t0.588511\t-0.005011\n",
      "20\t0.580150\t0.010970\n",
      "25\t0.592079\t-0.009504\n",
      "30\t0.588796\t-0.002602\n",
      "\n",
      "Step 5: Evaluating MINIBATCH model...\n",
      "\n",
      "MINIBATCH Validation Results:\n",
      "Accuracy:  0.9414\n",
      "Precision: 0.7518\n",
      "Recall:    0.8729\n",
      "F1-score:  0.8078\n",
      "\n",
      "MINIBATCH Test Results:\n",
      "Accuracy:  0.9498\n",
      "Precision: 0.7744\n",
      "Recall:    0.8957\n",
      "F1-score:  0.8306\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "def load_and_split_data(file_path, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Loads text data from a file and splits it into training, validation, and test sets.\n",
    "    Args:\n",
    "        file_path (str): Path to the data file\n",
    "        train_ratio (float): Proportion of data for training\n",
    "        val_ratio (float): Proportion of data for validation\n",
    "    Returns:\n",
    "        tuple: Contains train, validation, and test data and labels\n",
    "    \"\"\"\n",
    "    print(f\"Reading data from {file_path}...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"Total number of samples loaded: {len(lines)}\")\n",
    "    \n",
    "    # Count label distribution\n",
    "    label_counts = Counter(line.split('\\t')[0] for line in lines)\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"{label}: {count} ({count/len(lines)*100:.2f}%)\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    # Calculate split points\n",
    "    train_end = int(len(lines) * train_ratio)\n",
    "    val_end = int(len(lines) * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Split data\n",
    "    train_data = lines[:train_end]\n",
    "    val_data = lines[train_end:val_end]\n",
    "    test_data = lines[val_end:]\n",
    "    \n",
    "    def split_data(data):\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in data:\n",
    "            if '\\t' in line:\n",
    "                label, text = line.strip().split('\\t', 1)\n",
    "                # Convert 'spam' to 1 and 'ham' to 0\n",
    "                label_int = 1 if label.lower() == 'spam' else 0\n",
    "                labels.append(label_int)\n",
    "                sentences.append(text)\n",
    "        return sentences, labels\n",
    "    \n",
    "    return split_data(train_data) + split_data(val_data) + split_data(test_data)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing special characters and converting to lowercase.\n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = ' '.join(word for word in text.split() \n",
    "                   if not word.startswith(('http:', 'https:', 'www.')))\n",
    "    \n",
    "    # Keep only letters and spaces\n",
    "    cleaned_text = \"\"\n",
    "    for char in text:\n",
    "        if char.isalpha() or char.isspace():\n",
    "            cleaned_text += char\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Converts text into tokens, removes stopwords, and applies stemming.\n",
    "    Args:\n",
    "        text (str): Input text to tokenize\n",
    "    Returns:\n",
    "        list: List of processed tokens\n",
    "    \"\"\"\n",
    "    stopwords = {\n",
    "        \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\", \n",
    "        \"has\", \"he\", \"in\", \"is\", \"it\", \"its\", \"of\", \"on\", \"that\", \"the\", \n",
    "        \"to\", \"was\", \"were\", \"will\", \"with\", \"the\", \"this\", \"but\", \"they\",\n",
    "        \"have\", \"had\", \"what\", \"when\", \"where\", \"who\", \"which\", \"why\", \"how\"\n",
    "    }\n",
    "    \n",
    "    def simple_stem(word):\n",
    "        \"\"\"Simple word stemming rules\"\"\"\n",
    "        if len(word) < 4:\n",
    "            return word\n",
    "        if word.endswith('ing'):\n",
    "            return word[:-3]\n",
    "        elif word.endswith('ed'):\n",
    "            return word[:-2]\n",
    "        elif word.endswith('s'):\n",
    "            return word[:-1]\n",
    "        return word\n",
    "    \n",
    "    words = text.split()\n",
    "    return [simple_stem(word) for word in words \n",
    "            if word not in stopwords and len(word) > 1]\n",
    "\n",
    "class TfidfVectorizer:\n",
    "    \"\"\"\n",
    "    Converts text documents to TF-IDF feature vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocabulary = set()\n",
    "        self.idf = {}\n",
    "        self.vocab_index = {}\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"\n",
    "        Builds vocabulary and computes IDF scores from documents\n",
    "        Args:\n",
    "            documents (list): List of tokenized documents\n",
    "        \"\"\"\n",
    "        # Build vocabulary\n",
    "        for doc in documents:\n",
    "            self.vocabulary.update(doc)\n",
    "        \n",
    "        self.vocab_index = {word: idx for idx, word in enumerate(sorted(self.vocabulary))}\n",
    "        \n",
    "        # Compute document frequencies\n",
    "        doc_freq = Counter()\n",
    "        for doc in documents:\n",
    "            doc_words = set(doc)\n",
    "            for word in doc_words:\n",
    "                doc_freq[word] += 1\n",
    "        \n",
    "        # Calculate IDF scores\n",
    "        num_docs = len(documents)\n",
    "        self.idf = {word: math.log((num_docs + 1)/(doc_freq[word] + 1)) + 1 \n",
    "                   for word in self.vocabulary}\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"\n",
    "        Transforms documents into TF-IDF feature vectors\n",
    "        Args:\n",
    "            documents (list): List of tokenized documents\n",
    "        Returns:\n",
    "            list: List of TF-IDF feature vectors\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        for doc in documents:\n",
    "            tf = Counter(doc)\n",
    "            doc_len = len(doc) if len(doc) > 0 else 1\n",
    "            \n",
    "            features = [0.0] * len(self.vocabulary)\n",
    "            for word in set(doc):\n",
    "                if word in self.vocab_index:\n",
    "                    idx = self.vocab_index[word]\n",
    "                    tf_val = tf[word] / doc_len\n",
    "                    features[idx] = tf_val * self.idf.get(word, 0)\n",
    "            \n",
    "            X.append(features)\n",
    "        return X\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, lambda_reg=0.1, num_epochs=100, \n",
    "                 early_stop_threshold=1e-4, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "        self.training_history = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\"\"\"\n",
    "        # Clip z to prevent overflow\n",
    "        z = min(max(z, -100), 100)\n",
    "        return 1 / (1 + math.exp(-z))\n",
    "    \n",
    "    def compute_loss(self, X, y, indices):\n",
    "        \"\"\"Compute binary cross-entropy loss with L2 regularization\"\"\"\n",
    "        loss = 0\n",
    "        for i in indices:\n",
    "            z = sum(X[i][j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "            pred = self.sigmoid(z)\n",
    "            # Add small epsilon to avoid log(0)\n",
    "            loss -= (y[i] * math.log(pred + 1e-15) + (1 - y[i]) * math.log(1 - pred + 1e-15))\n",
    "        \n",
    "        # Add L2 regularization term\n",
    "        l2_term = self.lambda_reg * sum(w * w for w in self.weights)\n",
    "        return (loss / len(indices)) + l2_term\n",
    "    \n",
    "    def compute_class_weights(self, y):\n",
    "        \"\"\"\n",
    "        Compute class weights to handle class imbalance\n",
    "        Args:\n",
    "            y (list): Labels\n",
    "        Returns:\n",
    "            dict: Class weights\n",
    "        \"\"\"\n",
    "        counts = Counter(y)\n",
    "        total = len(y)\n",
    "        weights = {\n",
    "            0: total / (2 * counts[0]) if counts[0] > 0 else 1,\n",
    "            1: total / (2 * counts[1]) if counts[1] > 0 else 1\n",
    "        }\n",
    "        print(f\"Class weights: {weights}\")\n",
    "        return weights\n",
    "    \n",
    "    def train(self, X, y, method='sgd'):\n",
    "        \"\"\"\n",
    "        Train the model using either SGD or mini-batch gradient descent\n",
    "        Args:\n",
    "            X (list): Feature vectors\n",
    "            y (list): Labels\n",
    "            method (str): 'sgd' or 'minibatch'\n",
    "        \"\"\"\n",
    "        if not self.weights:\n",
    "            # Initialize weights with small random values\n",
    "            self.weights = [random.uniform(-0.1, 0.1) for _ in range(len(X[0]))]\n",
    "        \n",
    "        # Compute class weights for balanced training\n",
    "        class_weights = self.compute_class_weights(y)\n",
    "        \n",
    "        print(f\"\\nTraining with {method.upper()}:\")\n",
    "        print(\"Epoch\\tLoss\\t\\tΔLoss\")\n",
    "        \n",
    "        n_samples = len(y)\n",
    "        prev_loss = float('inf')\n",
    "        no_improvement_count = 0\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            indices = list(range(n_samples))\n",
    "            random.shuffle(indices)\n",
    "            \n",
    "            if method == 'sgd':\n",
    "                batch_indices = [[i] for i in indices]\n",
    "            else:\n",
    "                batch_indices = [indices[i:i + self.batch_size] \n",
    "                               for i in range(0, len(indices), self.batch_size)]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            for batch in batch_indices:\n",
    "                weight_gradients = [0] * len(self.weights)\n",
    "                bias_gradient = 0\n",
    "                \n",
    "                for i in batch:\n",
    "                    # Forward pass\n",
    "                    z = sum(X[i][j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "                    pred = self.sigmoid(z)\n",
    "                    \n",
    "                    # Apply class weights to error\n",
    "                    sample_weight = class_weights[y[i]]\n",
    "                    error = sample_weight * (pred - y[i])\n",
    "                    \n",
    "                    # Accumulate gradients\n",
    "                    for j in range(len(self.weights)):\n",
    "                        weight_gradients[j] += error * X[i][j]\n",
    "                    bias_gradient += error\n",
    "                \n",
    "                # Apply updates with regularization\n",
    "                batch_size = len(batch)\n",
    "                for j in range(len(self.weights)):\n",
    "                    reg_gradient = 2 * self.lambda_reg * self.weights[j]\n",
    "                    self.weights[j] -= self.learning_rate * (weight_gradients[j]/batch_size + reg_gradient)\n",
    "                self.bias -= self.learning_rate * (bias_gradient/batch_size)\n",
    "                \n",
    "                epoch_loss += self.compute_loss(X, y, batch)\n",
    "            \n",
    "            avg_loss = epoch_loss / len(batch_indices)\n",
    "            loss_change = prev_loss - avg_loss\n",
    "            self.training_history.append(avg_loss)\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"{epoch}\\t{avg_loss:.6f}\\t{loss_change:.6f}\")\n",
    "            \n",
    "            # Early stopping with patience\n",
    "            if abs(loss_change) < self.early_stop_threshold:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= 3:  # Wait for 3 epochs of no improvement\n",
    "                    print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            else:\n",
    "                no_improvement_count = 0\n",
    "            \n",
    "            prev_loss = avg_loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            z = sum(x[j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "            prob = self.sigmoid(z)\n",
    "            predictions.append(1 if prob >= 0.5 else 0)\n",
    "        return predictions\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics\n",
    "    Returns:\n",
    "        tuple: (accuracy, precision, recall, f1)\n",
    "    \"\"\"\n",
    "    tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n",
    "    tn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 0)\n",
    "    fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n",
    "    fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n",
    "    \n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def perform_cross_validation(X, y, lambda_values, k=5):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation to find the best regularization parameter\n",
    "    Args:\n",
    "        X (list): Feature vectors\n",
    "        y (list): Labels\n",
    "        lambda_values (list): List of lambda values to try\n",
    "        k (int): Number of folds\n",
    "    Returns:\n",
    "        float: Best lambda value\n",
    "    \"\"\"\n",
    "    print(\"\\nPerforming cross-validation...\")\n",
    "    n_samples = len(y)\n",
    "    fold_size = n_samples // k\n",
    "    best_lambda = None\n",
    "    best_score = -float('inf')\n",
    "    \n",
    "    for lambda_val in lambda_values:\n",
    "        print(f\"\\nTrying lambda = {lambda_val}\")\n",
    "        scores = []\n",
    "        \n",
    "        for fold in range(k):\n",
    "            # Split data into training and validation\n",
    "            val_start = fold * fold_size\n",
    "            val_end = val_start + fold_size\n",
    "            \n",
    "            X_val = X[val_start:val_end]\n",
    "            y_val = y[val_start:val_end]\n",
    "            X_train = X[:val_start] + X[val_end:]\n",
    "            y_train = y[:val_start] + y[val_end:]\n",
    "            \n",
    "            # Train model\n",
    "            model = LogisticRegression(lambda_reg=lambda_val, num_epochs=50)\n",
    "            model.train(X_train, y_train, method='minibatch')\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_val)\n",
    "            accuracy, _, _, f1 = evaluate(y_val, y_pred)\n",
    "            scores.append(f1)\n",
    "        \n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        print(f\"Average F1-score: {avg_score:.4f}\")\n",
    "        \n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_lambda = lambda_val\n",
    "    \n",
    "    print(f\"\\nBest lambda value: {best_lambda} (F1-score: {best_score:.4f})\")\n",
    "    return best_lambda\n",
    "\n",
    "def main():\n",
    "    # Example usage with a sample input file\n",
    "    # Assuming input file format: label\\ttext\n",
    "    file_path = \"SMSSpamCollection\"  # Replace with your data file path\n",
    "    \n",
    "    # Step 1: Load and split data\n",
    "    train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = load_and_split_data(file_path)\n",
    "    \n",
    "    # Step 2: Preprocess and tokenize texts\n",
    "    print(\"\\nPreprocessing and tokenizing texts...\")\n",
    "    train_tokens = [tokenize(preprocess_text(text)) for text in train_texts]\n",
    "    val_tokens = [tokenize(preprocess_text(text)) for text in val_texts]\n",
    "    test_tokens = [tokenize(preprocess_text(text)) for text in test_texts]\n",
    "    \n",
    "    # Step 3: Create TF-IDF features\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(train_tokens)\n",
    "    \n",
    "    X_train = vectorizer.transform(train_tokens)\n",
    "    X_val = vectorizer.transform(val_tokens)\n",
    "    X_test = vectorizer.transform(test_tokens)\n",
    "    \n",
    "    # Step 4: Train and evaluate with both SGD and mini-batch\n",
    "    for method in ['sgd', 'minibatch']:\n",
    "        print(f\"\\nStep 4: Training with {method.upper()}...\")\n",
    "        model = LogisticRegression(\n",
    "            learning_rate=0.1,  # Increased learning rate\n",
    "            lambda_reg=0.01,    # Reduced regularization\n",
    "            num_epochs=35,\n",
    "            early_stop_threshold=1e-4,\n",
    "            batch_size=32\n",
    "        )\n",
    "        model.train(X_train, train_labels, method=method)\n",
    "        \n",
    "        # Evaluate on both validation and test sets\n",
    "        print(f\"\\nStep 5: Evaluating {method.upper()} model...\")\n",
    "        \n",
    "        # Validation set evaluation\n",
    "        val_pred = model.predict(X_val)\n",
    "        val_accuracy, val_precision, val_recall, val_f1 = evaluate(val_labels, val_pred)\n",
    "        print(f\"\\n{method.upper()} Validation Results:\")\n",
    "        print(f\"Accuracy:  {val_accuracy:.4f}\")\n",
    "        print(f\"Precision: {val_precision:.4f}\")\n",
    "        print(f\"Recall:    {val_recall:.4f}\")\n",
    "        print(f\"F1-score:  {val_f1:.4f}\")\n",
    "        \n",
    "        # Test set evaluation\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_accuracy, test_precision, test_recall, test_f1 = evaluate(test_labels, test_pred)\n",
    "        print(f\"\\n{method.upper()} Test Results:\")\n",
    "        print(f\"Accuracy:  {test_accuracy:.4f}\")\n",
    "        print(f\"Precision: {test_precision:.4f}\")\n",
    "        print(f\"Recall:    {test_recall:.4f}\")\n",
    "        print(f\"F1-score:  {test_f1:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Part A: SMS Spam Classification ===\n",
      "Reading data from SMSSpamCollection...\n",
      "\n",
      "Label Distribution:\n",
      "Label 0: 4827 (86.60%)\n",
      "Label 1: 747 (13.40%)\n",
      "\n",
      "Training model:\n",
      "Epoch\tLoss\n",
      "0\t0.400881\n",
      "1\t0.381376\n",
      "2\t0.370342\n",
      "3\t0.360767\n",
      "4\t0.351853\n",
      "5\t0.343650\n",
      "6\t0.336129\n",
      "7\t0.329191\n",
      "8\t0.322815\n",
      "9\t0.316977\n",
      "10\t0.311522\n",
      "11\t0.306603\n",
      "12\t0.301992\n",
      "13\t0.297788\n",
      "14\t0.293920\n",
      "15\t0.290357\n",
      "16\t0.287081\n",
      "17\t0.284031\n",
      "18\t0.281237\n",
      "19\t0.278658\n",
      "\n",
      "Training model:\n",
      "Epoch\tLoss\n",
      "0\t0.410443\n",
      "1\t0.391494\n",
      "2\t0.379805\n",
      "3\t0.369559\n",
      "4\t0.360140\n",
      "5\t0.351446\n",
      "6\t0.343471\n",
      "7\t0.336136\n",
      "8\t0.329373\n",
      "9\t0.323154\n",
      "10\t0.317463\n",
      "11\t0.312205\n",
      "12\t0.307399\n",
      "13\t0.302967\n",
      "14\t0.298888\n",
      "15\t0.295157\n",
      "16\t0.291667\n",
      "17\t0.288487\n",
      "18\t0.285563\n",
      "19\t0.282833\n",
      "\n",
      "Training model:\n",
      "Epoch\tLoss\n",
      "0\t0.409711\n",
      "1\t0.389248\n",
      "2\t0.377944\n",
      "3\t0.368101\n",
      "4\t0.359042\n",
      "5\t0.350722\n",
      "6\t0.342988\n",
      "7\t0.335897\n",
      "8\t0.329375\n",
      "9\t0.323353\n",
      "10\t0.317803\n",
      "11\t0.312721\n",
      "12\t0.308035\n",
      "13\t0.303728\n",
      "14\t0.299751\n",
      "15\t0.296055\n",
      "16\t0.292677\n",
      "17\t0.289539\n",
      "18\t0.286658\n",
      "19\t0.284013\n",
      "\n",
      "Training model:\n",
      "Epoch\tLoss\n",
      "0\t0.408124\n",
      "1\t0.387195\n",
      "2\t0.375940\n",
      "3\t0.365847\n",
      "4\t0.356600\n",
      "5\t0.348113\n",
      "6\t0.340300\n",
      "7\t0.333097\n",
      "8\t0.326484\n",
      "9\t0.320385\n",
      "10\t0.314783\n",
      "11\t0.309680\n",
      "12\t0.304968\n",
      "13\t0.300615\n",
      "14\t0.296601\n",
      "15\t0.292954\n",
      "16\t0.289546\n",
      "17\t0.286422\n",
      "18\t0.283494\n",
      "19\t0.280831\n",
      "\n",
      "Reached maximum training time limit\n",
      "\n",
      "Training model:\n",
      "Epoch\tLoss\n",
      "\n",
      "Error occurred: unsupported operand type(s) for *: 'int' and 'NoneType'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 562\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 562\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 498\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;66;03m# Train final model with best lambda\u001b[39;00m\n\u001b[0;32m    490\u001b[0m final_model \u001b[38;5;241m=\u001b[39m LogisticRegression(\n\u001b[0;32m    491\u001b[0m     num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vectorizer\u001b[38;5;241m.\u001b[39mvocabulary),\n\u001b[0;32m    492\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    495\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m   \u001b[38;5;66;03m# Smaller batch size\u001b[39;00m\n\u001b[0;32m    496\u001b[0m )\n\u001b[1;32m--> 498\u001b[0m \u001b[43mfinal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[0;32m    501\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m final_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[1;32mIn[14], line 227\u001b[0m, in \u001b[0;36mLogisticRegression.train\u001b[1;34m(self, X, y, early_stopping_patience, min_delta)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 227\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_binary_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_multiclass_batch(X, y, batch)\n",
      "Cell \u001b[1;32mIn[14], line 265\u001b[0m, in \u001b[0;36mLogisticRegression._train_binary_batch\u001b[1;34m(self, X, y, batch)\u001b[0m\n\u001b[0;32m    263\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;241m0\u001b[39m])):\n\u001b[1;32m--> 265\u001b[0m     reg_term \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlambda_reg\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;241m0\u001b[39m][j]\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;241m0\u001b[39m][j] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m (grad_w[j]\u001b[38;5;241m/\u001b[39mbatch_size \u001b[38;5;241m+\u001b[39m reg_term)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m (grad_b\u001b[38;5;241m/\u001b[39mbatch_size)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def load_and_split_data(file_path, is_sms=True, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Loads text data from a file and splits it into training, validation, and test sets.\n",
    "    Works for both SMS and author classification tasks.\n",
    "    \"\"\"\n",
    "    print(f\"Reading data from {file_path}...\")\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    if is_sms:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if '\\t' in line:\n",
    "                    label, text = line.strip().split('\\t', 1)\n",
    "                    label_int = 1 if label.lower() == 'spam' else 0\n",
    "                    labels.append(label_int)\n",
    "                    texts.append(text)\n",
    "    else:\n",
    "        author_map = {\"Arthur Conan Doyle\": 0, \"Fyodor Dostoyevsky\": 1, \"Jane Austen\": 2}\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if '\\t' in line:\n",
    "                    author, text = line.strip().split('\\t', 1)\n",
    "                    if author in author_map:\n",
    "                        labels.append(author_map[author])\n",
    "                        texts.append(text)\n",
    "    \n",
    "    if not texts or not labels:\n",
    "        raise ValueError(\"No valid data found in the input file\")\n",
    "    \n",
    "    # Print data distribution\n",
    "    label_counts = Counter(labels)\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        print(f\"Label {label}: {count} ({count/len(labels)*100:.2f}%)\")\n",
    "    \n",
    "    # Shuffle and split data\n",
    "    combined = list(zip(texts, labels))\n",
    "    random.seed(42)\n",
    "    random.shuffle(combined)\n",
    "    texts, labels = zip(*combined)\n",
    "    \n",
    "    train_end = int(len(texts) * train_ratio)\n",
    "    val_end = int(len(texts) * (train_ratio + val_ratio))\n",
    "    \n",
    "    if train_end == 0 or val_end == train_end:\n",
    "        raise ValueError(\"Dataset too small for the specified split ratios\")\n",
    "    \n",
    "    return (\n",
    "        (list(texts[:train_end]), list(labels[:train_end])),\n",
    "        (list(texts[train_end:val_end]), list(labels[train_end:val_end])),\n",
    "        (list(texts[val_end:]), list(labels[val_end:]))\n",
    "    )\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans text by removing special characters and converting to lowercase.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = ' '.join(word for word in text.split() \n",
    "                   if not word.startswith(('http:', 'https:', 'www.')))\n",
    "    \n",
    "    # Keep only letters and spaces\n",
    "    cleaned_text = \"\"\n",
    "    for char in text:\n",
    "        if char.isalpha() or char.isspace():\n",
    "            cleaned_text += char\n",
    "    \n",
    "    return ' '.join(cleaned_text.split())  # Normalize whitespace\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Converts text into tokens, removes stopwords, and applies stemming.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "        \n",
    "    stopwords = {\n",
    "        \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\", \n",
    "        \"has\", \"he\", \"in\", \"is\", \"it\", \"its\", \"of\", \"on\", \"that\", \"the\", \n",
    "        \"to\", \"was\", \"were\", \"will\", \"with\", \"the\", \"this\", \"but\", \"they\",\n",
    "        \"have\", \"had\", \"what\", \"when\", \"where\", \"who\", \"which\", \"why\", \"how\"\n",
    "    }\n",
    "    \n",
    "    def simple_stem(word):\n",
    "        \"\"\"Simple word stemming rules\"\"\"\n",
    "        if not word or len(word) < 4:\n",
    "            return word\n",
    "        if word.endswith('ing'):\n",
    "            return word[:-3]\n",
    "        elif word.endswith('ed'):\n",
    "            return word[:-2]\n",
    "        elif word.endswith('s'):\n",
    "            return word[:-1]\n",
    "        return word\n",
    "    \n",
    "    words = text.split()\n",
    "    return [simple_stem(word) for word in words \n",
    "            if word and word not in stopwords and len(word) > 1]\n",
    "\n",
    "class TfidfVectorizer:\n",
    "    \"\"\"Converts text documents to TF-IDF feature vectors.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocabulary = set()\n",
    "        self.idf = {}\n",
    "        self.vocab_index = {}\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"Builds vocabulary and computes IDF scores from documents\"\"\"\n",
    "        if not documents:\n",
    "            raise ValueError(\"No documents provided for fitting\")\n",
    "            \n",
    "        # Build vocabulary\n",
    "        for doc in documents:\n",
    "            if doc:  # Skip empty documents\n",
    "                self.vocabulary.update(doc)\n",
    "        \n",
    "        if not self.vocabulary:\n",
    "            raise ValueError(\"No valid terms found in documents\")\n",
    "            \n",
    "        self.vocab_index = {word: idx for idx, word in enumerate(sorted(self.vocabulary))}\n",
    "        \n",
    "        # Compute document frequencies\n",
    "        doc_freq = Counter()\n",
    "        for doc in documents:\n",
    "            doc_words = set(doc)\n",
    "            for word in doc_words:\n",
    "                doc_freq[word] += 1\n",
    "        \n",
    "        # Calculate IDF scores\n",
    "        num_docs = len(documents)\n",
    "        self.idf = {word: math.log((num_docs + 1)/(doc_freq[word] + 1)) + 1 \n",
    "                   for word in self.vocabulary}\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"Transforms documents into TF-IDF feature vectors\"\"\"\n",
    "        if not documents:\n",
    "            raise ValueError(\"No documents provided for transformation\")\n",
    "            \n",
    "        if not self.vocabulary:\n",
    "            raise ValueError(\"Vectorizer needs to be fitted before transform\")\n",
    "            \n",
    "        X = []\n",
    "        for doc in documents:\n",
    "            tf = Counter(doc)\n",
    "            doc_len = len(doc) if doc else 1\n",
    "            \n",
    "            features = [0.0] * len(self.vocabulary)\n",
    "            for word in set(doc):\n",
    "                if word in self.vocab_index:\n",
    "                    idx = self.vocab_index[word]\n",
    "                    tf_val = tf[word] / doc_len\n",
    "                    features[idx] = tf_val * self.idf.get(word, 0)\n",
    "            \n",
    "            X.append(features)\n",
    "        return X\n",
    "\n",
    "class LogisticRegression:\n",
    "    \"\"\"Logistic Regression with support for both binary and multiclass classification.\"\"\"\n",
    "    def __init__(self, num_features, num_classes=2, learning_rate=0.1, lambda_reg=0.01, \n",
    "                 num_epochs=50, batch_size=64):\n",
    "        if num_features <= 0:\n",
    "            raise ValueError(\"Number of features must be positive\")\n",
    "        if num_classes < 2:\n",
    "            raise ValueError(\"Number of classes must be at least 2\")\n",
    "        if learning_rate <= 0:\n",
    "            raise ValueError(\"Learning rate must be positive\")\n",
    "        # if lambda_reg < 0:\n",
    "        #     raise ValueError(\"Lambda regularization must be non-negative\")\n",
    "            \n",
    "        self.num_classes = num_classes\n",
    "        self.weights = [[random.uniform(-0.1, 0.1) for _ in range(num_features)] \n",
    "                       for _ in range(num_classes if num_classes > 2 else 1)]\n",
    "        self.biases = [0] * (num_classes if num_classes > 2 else 1)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function with overflow protection\"\"\"\n",
    "        if z < -709:  # np.log(np.finfo(np.float64).tiny)\n",
    "            return 0.0\n",
    "        elif z > 709:  # np.log(np.finfo(np.float64).max)\n",
    "            return 1.0\n",
    "        return 1 / (1 + math.exp(-z))\n",
    "\n",
    "    def softmax(self, scores):\n",
    "        \"\"\"Compute softmax probabilities with overflow protection\"\"\"\n",
    "        max_score = max(scores)\n",
    "        exp_scores = [math.exp(score - max_score) for score in scores]\n",
    "        total = sum(exp_scores)\n",
    "        if total == 0:\n",
    "            return [1.0/len(scores)] * len(scores)\n",
    "        return [exp_score / total for exp_score in exp_scores]\n",
    "\n",
    "    def train(self, X, y, early_stopping_patience=3, min_delta=0.001):\n",
    "        \"\"\"Train with early stopping\"\"\"\n",
    "        if not X or not y:\n",
    "            raise ValueError(\"Empty training data\")\n",
    "        if len(X) != len(y):\n",
    "            raise ValueError(\"Features and labels must have the same length\")\n",
    "            \n",
    "        n_samples = len(y)\n",
    "        print(\"\\nTraining model:\")\n",
    "        print(\"Epoch\\tLoss\")\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            indices = list(range(n_samples))\n",
    "            random.shuffle(indices)\n",
    "            \n",
    "            # Create mini-batches\n",
    "            batches = [indices[i:i + self.batch_size] \n",
    "                      for i in range(0, len(indices), self.batch_size)]\n",
    "            \n",
    "            for batch in batches:\n",
    "                if self.num_classes == 2:\n",
    "                    self._train_binary_batch(X, y, batch)\n",
    "                else:\n",
    "                    self._train_multiclass_batch(X, y, batch)\n",
    "            \n",
    "            # Calculate loss\n",
    "            current_loss = self._calculate_loss(X, y)\n",
    "            print(f\"{epoch}\\t{current_loss:.6f}\")\n",
    "            \n",
    "            # Early stopping check\n",
    "            if current_loss < best_loss - min_delta:\n",
    "                best_loss = current_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    def _train_binary_batch(self, X, y, batch):\n",
    "        \"\"\"Train a batch for binary classification\"\"\"\n",
    "        grad_w = [0] * len(self.weights[0])\n",
    "        grad_b = 0\n",
    "        \n",
    "        for idx in batch:\n",
    "            # Forward pass\n",
    "            z = sum(X[idx][j] * self.weights[0][j] for j in range(len(X[idx]))) + self.biases[0]\n",
    "            y_pred = self.sigmoid(z)\n",
    "            \n",
    "            # Gradient computation\n",
    "            error = y_pred - y[idx]\n",
    "            for j in range(len(X[idx])):\n",
    "                grad_w[j] += error * X[idx][j]\n",
    "            grad_b += error\n",
    "        \n",
    "        # Update weights and bias\n",
    "        batch_size = len(batch)\n",
    "        for j in range(len(self.weights[0])):\n",
    "            reg_term = 2 * self.lambda_reg * self.weights[0][j]\n",
    "            self.weights[0][j] -= self.learning_rate * (grad_w[j]/batch_size + reg_term)\n",
    "        self.biases[0] -= self.learning_rate * (grad_b/batch_size)\n",
    "\n",
    "    def _train_multiclass_batch(self, X, y, batch):\n",
    "        \"\"\"Train a batch for multiclass classification with class weights\"\"\"\n",
    "        # Calculate class weights\n",
    "        class_counts = Counter(y)\n",
    "        total_samples = len(y)\n",
    "        class_weights = {c: total_samples / (len(class_counts) * count) \n",
    "                        for c, count in class_counts.items()}\n",
    "        \n",
    "        gradients = [[0] * len(self.weights[0]) for _ in range(self.num_classes)]\n",
    "        bias_grads = [0] * self.num_classes\n",
    "        \n",
    "        for idx in batch:\n",
    "            # Forward pass\n",
    "            scores = [sum(X[idx][j] * self.weights[c][j] for j in range(len(X[idx]))) + self.biases[c]\n",
    "                     for c in range(self.num_classes)]\n",
    "            probs = self.softmax(scores)\n",
    "            \n",
    "            # Gradient computation with class weights\n",
    "            weight = class_weights[y[idx]]\n",
    "            for c in range(self.num_classes):\n",
    "                error = probs[c]\n",
    "                if c == y[idx]:\n",
    "                    error -= 1\n",
    "                error *= weight\n",
    "                \n",
    "                for j in range(len(X[idx])):\n",
    "                    gradients[c][j] += error * X[idx][j]\n",
    "                bias_grads[c] += error\n",
    "        \n",
    "        # Update weights and biases\n",
    "        batch_size = len(batch)\n",
    "        for c in range(self.num_classes):\n",
    "            for j in range(len(self.weights[c])):\n",
    "                reg_term = 2 * self.lambda_reg * self.weights[c][j]\n",
    "                self.weights[c][j] -= self.learning_rate * (gradients[c][j]/batch_size + reg_term)\n",
    "            self.biases[c] -= self.learning_rate * (bias_grads[c]/batch_size)\n",
    "\n",
    "    def _calculate_loss(self, X, y):\n",
    "        \"\"\"Calculate the total loss\"\"\"\n",
    "        loss = 0\n",
    "        epsilon = 1e-15  # Small constant to prevent log(0)\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            if self.num_classes == 2:\n",
    "                z = sum(X[i][j] * self.weights[0][j] for j in range(len(X[i]))) + self.biases[0]\n",
    "                y_pred = self.sigmoid(z)\n",
    "                y_pred = max(min(y_pred, 1 - epsilon), epsilon)  # Clip predictions\n",
    "                loss -= y[i] * math.log(y_pred) + (1 - y[i]) * math.log(1 - y_pred)\n",
    "            else:\n",
    "                scores = [sum(X[i][j] * self.weights[c][j] for j in range(len(X[i]))) + self.biases[c]\n",
    "                         for c in range(self.num_classes)]\n",
    "                probs = self.softmax(scores)\n",
    "                loss -= math.log(max(probs[y[i]], epsilon))\n",
    "        \n",
    "        # Add regularization term\n",
    "        reg_term = sum(sum(w * w for w in class_weights) for class_weights in self.weights)\n",
    "        loss = loss/len(X) + self.lambda_reg * reg_term\n",
    "        return loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for input data\"\"\"\n",
    "        if not X:\n",
    "            raise ValueError(\"Empty prediction data\")\n",
    "            \n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            if self.num_classes == 2:\n",
    "                z = sum(x[j] * self.weights[0][j] for j in range(len(x))) + self.biases[0]\n",
    "                predictions.append(1 if self.sigmoid(z) >= 0.5 else 0)\n",
    "            else:\n",
    "                scores = [sum(x[j] * self.weights[c][j] for j in range(len(x))) + self.biases[c]\n",
    "                         for c in range(self.num_classes)]\n",
    "                predictions.append(scores.index(max(scores)))\n",
    "        return predictions\n",
    "\n",
    "def evaluate(y_true, y_pred, num_classes=2):\n",
    "    \"\"\"Compute evaluation metrics for both binary and multiclass classification\"\"\"\n",
    "    if not y_true or not y_pred:\n",
    "        raise ValueError(\"Empty evaluation data\")\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"Predictions and true labels must have the same length\")\n",
    "        \n",
    "    if num_classes == 2:\n",
    "        tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n",
    "        tn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 0)\n",
    "        fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n",
    "        fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n",
    "        \n",
    "        accuracy = (tp + tn) / len(y_true)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return accuracy, precision, recall, f1\n",
    "    else:\n",
    "        confusion_matrix = [[0] * num_classes for _ in range(num_classes)]\n",
    "        for yt, yp in zip(y_true, y_pred):\n",
    "            confusion_matrix[yt][yp] += 1\n",
    "        \n",
    "        accuracy = sum(confusion_matrix[i][i] for i in range(num_classes)) / len(y_true)\n",
    "        \n",
    "        precision = []\n",
    "        recall = []\n",
    "        for c in range(num_classes):\n",
    "            true_pos = confusion_matrix[c][c]\n",
    "            false_pos = sum(confusion_matrix[i][c] for i in range(num_classes)) - true_pos\n",
    "            false_neg = sum(confusion_matrix[c][i] for i in range(num_classes)) - true_pos\n",
    "            \n",
    "            p = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
    "            r = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "            precision.append(p)\n",
    "            recall.append(r)\n",
    "        \n",
    "        macro_precision = sum(precision) / num_classes\n",
    "        macro_recall = sum(recall) / num_classes\n",
    "        macro_f1 = 2 * (macro_precision * macro_recall) / (macro_precision + macro_recall) \\\n",
    "                   if (macro_precision + macro_recall) > 0 else 0\n",
    "        \n",
    "        return accuracy, macro_precision, macro_recall, macro_f1\n",
    "\n",
    "def run_with_cross_validation(X, y, num_features, num_classes, lambda_values, max_train_time=300):\n",
    "    \"\"\"Run cross-validation with time limit and convergence detection\"\"\"\n",
    "    import time\n",
    "    \n",
    "    k_folds = 5\n",
    "    best_lambda = None\n",
    "    best_val_f1 = -1\n",
    "    convergence_threshold = 0.001  # Consider converged if improvement less than this\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for lambda_reg in lambda_values:\n",
    "        fold_f1s = []\n",
    "        \n",
    "        # K-fold cross validation\n",
    "        fold_size = len(X) // k_folds\n",
    "        for k in range(k_folds):\n",
    "            if time.time() - start_time > max_train_time:\n",
    "                print(\"\\nReached maximum training time limit\")\n",
    "                return best_lambda\n",
    "                \n",
    "            start_idx = k * fold_size\n",
    "            end_idx = start_idx + fold_size\n",
    "            \n",
    "            X_train = X[:start_idx] + X[end_idx:]\n",
    "            y_train = y[:start_idx] + y[end_idx:]\n",
    "            X_val = X[start_idx:end_idx]\n",
    "            y_val = y[start_idx:end_idx]\n",
    "            \n",
    "            model = LogisticRegression(\n",
    "                num_features=num_features,\n",
    "                num_classes=num_classes,\n",
    "                lambda_reg=lambda_reg,\n",
    "                num_epochs=20,\n",
    "                batch_size=32  # Smaller batch size for faster iterations\n",
    "            )\n",
    "            \n",
    "            model.train(X_train, y_train)\n",
    "            predictions = model.predict(X_val)\n",
    "            _, _, _, f1 = evaluate(y_val, predictions, num_classes)\n",
    "            fold_f1s.append(f1)\n",
    "        \n",
    "        avg_f1 = sum(fold_f1s) / len(fold_f1s)\n",
    "        print(f\"\\nλ={lambda_reg}: Average F1={avg_f1:.4f}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if avg_f1 > best_val_f1 + convergence_threshold:\n",
    "            best_val_f1 = avg_f1\n",
    "            best_lambda = lambda_reg\n",
    "        elif avg_f1 >= best_val_f1 - convergence_threshold:\n",
    "            # If we're within threshold of best, prefer smaller lambda\n",
    "            if best_lambda is None or lambda_reg < best_lambda:\n",
    "                best_lambda = lambda_reg\n",
    "                best_val_f1 = avg_f1\n",
    "    \n",
    "    print(f\"\\nBest lambda value from {k_folds}-fold cross-validation: {best_lambda}\")\n",
    "    return best_lambda\n",
    "\n",
    "def process_dataset(texts, vectorizer=None):\n",
    "    \"\"\"Process text data through preprocessing and feature extraction\"\"\"\n",
    "    # Preprocess texts\n",
    "    processed_texts = [preprocess_text(text) for text in texts]\n",
    "    tokenized_texts = [tokenize(text) for text in processed_texts]\n",
    "    \n",
    "    # Create or use existing vectorizer\n",
    "    if vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectorizer.fit(tokenized_texts)\n",
    "    \n",
    "    # Transform texts to feature vectors\n",
    "    X = vectorizer.transform(tokenized_texts)\n",
    "    \n",
    "    return X, vectorizer\n",
    "\n",
    "def main():\n",
    "    # Part A: Binary Classification (SMS Spam)\n",
    "    print(\"\\n=== Part A: SMS Spam Classification ===\")\n",
    "    \n",
    "    try:\n",
    "        # Load SMS data\n",
    "        (train_texts, train_labels), (val_texts, val_labels), (test_texts, test_labels) = \\\n",
    "            load_and_split_data(\"SMSSpamCollection\", is_sms=True)\n",
    "        \n",
    "        # Process training data\n",
    "        X_train, vectorizer = process_dataset(train_texts)\n",
    "        \n",
    "        # Process validation and test data using the same vectorizer\n",
    "        X_val, _ = process_dataset(val_texts, vectorizer)\n",
    "        X_test, _ = process_dataset(test_texts, vectorizer)\n",
    "        \n",
    "        # Cross-validation for lambda selection with 5-minute time limit\n",
    "        lambda_values = [0.001, 0.01, 0.1, 1.0]\n",
    "        best_lambda = run_with_cross_validation(\n",
    "            X_train, train_labels, \n",
    "            num_features=len(vectorizer.vocabulary),\n",
    "            num_classes=2,\n",
    "            lambda_values=lambda_values,\n",
    "            max_train_time=300  # 5 minutes maximum\n",
    "        )\n",
    "        \n",
    "        # Train final model with best lambda\n",
    "        final_model = LogisticRegression(\n",
    "            num_features=len(vectorizer.vocabulary),\n",
    "            num_classes=2,\n",
    "            lambda_reg=best_lambda,\n",
    "            num_epochs=20,  # Reduced epochs\n",
    "            batch_size=32   # Smaller batch size\n",
    "        )\n",
    "        \n",
    "        final_model.train(X_train, train_labels)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_predictions = final_model.predict(X_test)\n",
    "        accuracy, precision, recall, f1 = evaluate(test_labels, test_predictions, num_classes=2)\n",
    "        \n",
    "        print(\"\\nFinal SMS Classification Results:\")\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Test Precision: {precision:.4f}\")\n",
    "        print(f\"Test Recall: {recall:.4f}\")\n",
    "        print(f\"Test F1: {f1:.4f}\")\n",
    "        \n",
    "        # Part B: Multi-class Classification (Author Attribution)\n",
    "        print(\"\\n=== Part B: Author Attribution ===\")\n",
    "        \n",
    "        # Load author data\n",
    "        (train_texts, train_labels), (val_texts, val_labels), (test_texts, test_labels) = \\\n",
    "            load_and_split_data(\"books.txt\", is_sms=False)\n",
    "        \n",
    "        # Process training data\n",
    "        X_train, vectorizer = process_dataset(train_texts)\n",
    "        \n",
    "        # Process validation and test data using the same vectorizer\n",
    "        X_val, _ = process_dataset(val_texts, vectorizer)\n",
    "        X_test, _ = process_dataset(test_texts, vectorizer)\n",
    "        \n",
    "        # Cross-validation for lambda selection with 5-minute time limit\n",
    "        best_lambda = run_with_cross_validation(\n",
    "            X_train, train_labels,\n",
    "            num_features=len(vectorizer.vocabulary),\n",
    "            num_classes=3,\n",
    "            lambda_values=lambda_values,\n",
    "            max_train_time=300\n",
    "        )\n",
    "        \n",
    "        # Train final model with best lambda\n",
    "        final_model = LogisticRegression(\n",
    "            num_features=len(vectorizer.vocabulary),\n",
    "            num_classes=3,\n",
    "            learning_rate=0.2,\n",
    "            lambda_reg=best_lambda,\n",
    "            num_epochs=20,  # Reduced epochs\n",
    "            batch_size=32   # Smaller batch size\n",
    "        )\n",
    "        \n",
    "        final_model.train(X_train, train_labels)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_predictions = final_model.predict(X_test)\n",
    "        accuracy, macro_precision, macro_recall, macro_f1 = evaluate(\n",
    "            test_labels, test_predictions, num_classes=3\n",
    "        )\n",
    "        \n",
    "        print(\"\\nFinal Author Attribution Results:\")\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Test Macro-Precision: {macro_precision:.4f}\")\n",
    "        print(f\"Test Macro-Recall: {macro_recall:.4f}\")\n",
    "        print(f\"Test Macro-F1: {macro_f1:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
