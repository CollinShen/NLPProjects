{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from SMSSpamCollection...\n",
      "Total number of samples loaded: 5574\n",
      "\n",
      "Label Distribution:\n",
      "ham: 4827 (86.60%)\n",
      "spam: 747 (13.40%)\n",
      "\n",
      "Preprocessing and tokenizing texts...\n",
      "\n",
      "Step 4: Training with SGD...\n",
      "Class weights: {0: 0.5758783584292885, 1: 3.794747081712062}\n",
      "\n",
      "Training with SGD:\n",
      "Epoch\tLoss\t\tΔLoss\n",
      "0\t0.565894\tinf\n",
      "5\t0.548030\t0.004362\n",
      "10\t0.551706\t0.003164\n",
      "15\t0.549920\t0.003600\n",
      "20\t0.551242\t0.002120\n",
      "25\t0.552730\t-0.001959\n",
      "30\t0.554134\t-0.004919\n",
      "\n",
      "Step 5: Evaluating SGD model...\n",
      "\n",
      "SGD Validation Results:\n",
      "Accuracy:  0.9187\n",
      "Precision: 0.6524\n",
      "Recall:    0.9068\n",
      "F1-score:  0.7589\n",
      "\n",
      "SGD Test Results:\n",
      "Accuracy:  0.9283\n",
      "Precision: 0.6821\n",
      "Recall:    0.8957\n",
      "F1-score:  0.7744\n",
      "\n",
      "Step 4: Training with MINIBATCH...\n",
      "Class weights: {0: 0.5758783584292885, 1: 3.794747081712062}\n",
      "\n",
      "Training with MINIBATCH:\n",
      "Epoch\tLoss\t\tΔLoss\n",
      "0\t0.826326\tinf\n",
      "5\t0.604667\t0.011946\n",
      "10\t0.589657\t0.001560\n",
      "15\t0.588511\t-0.005011\n",
      "20\t0.580150\t0.010970\n",
      "25\t0.592079\t-0.009504\n",
      "30\t0.588796\t-0.002602\n",
      "\n",
      "Step 5: Evaluating MINIBATCH model...\n",
      "\n",
      "MINIBATCH Validation Results:\n",
      "Accuracy:  0.9414\n",
      "Precision: 0.7518\n",
      "Recall:    0.8729\n",
      "F1-score:  0.8078\n",
      "\n",
      "MINIBATCH Test Results:\n",
      "Accuracy:  0.9498\n",
      "Precision: 0.7744\n",
      "Recall:    0.8957\n",
      "F1-score:  0.8306\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "def load_and_split_data(file_path, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Loads text data from a file and splits it into training, validation, and test sets.\n",
    "    Args:\n",
    "        file_path (str): Path to the data file\n",
    "        train_ratio (float): Proportion of data for training\n",
    "        val_ratio (float): Proportion of data for validation\n",
    "    Returns:\n",
    "        tuple: Contains train, validation, and test data and labels\n",
    "    \"\"\"\n",
    "    print(f\"Reading data from {file_path}...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"Total number of samples loaded: {len(lines)}\")\n",
    "    \n",
    "    # Count label distribution\n",
    "    label_counts = Counter(line.split('\\t')[0] for line in lines)\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"{label}: {count} ({count/len(lines)*100:.2f}%)\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    # Calculate split points\n",
    "    train_end = int(len(lines) * train_ratio)\n",
    "    val_end = int(len(lines) * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Split data\n",
    "    train_data = lines[:train_end]\n",
    "    val_data = lines[train_end:val_end]\n",
    "    test_data = lines[val_end:]\n",
    "    \n",
    "    def split_data(data):\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in data:\n",
    "            if '\\t' in line:\n",
    "                label, text = line.strip().split('\\t', 1)\n",
    "                # Convert 'spam' to 1 and 'ham' to 0\n",
    "                label_int = 1 if label.lower() == 'spam' else 0\n",
    "                labels.append(label_int)\n",
    "                sentences.append(text)\n",
    "        return sentences, labels\n",
    "    \n",
    "    return split_data(train_data) + split_data(val_data) + split_data(test_data)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing special characters and converting to lowercase.\n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = ' '.join(word for word in text.split() \n",
    "                   if not word.startswith(('http:', 'https:', 'www.')))\n",
    "    \n",
    "    # Keep only letters and spaces\n",
    "    cleaned_text = \"\"\n",
    "    for char in text:\n",
    "        if char.isalpha() or char.isspace():\n",
    "            cleaned_text += char\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Converts text into tokens, removes stopwords, and applies stemming.\n",
    "    Args:\n",
    "        text (str): Input text to tokenize\n",
    "    Returns:\n",
    "        list: List of processed tokens\n",
    "    \"\"\"\n",
    "    stopwords = {\n",
    "        \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\", \n",
    "        \"has\", \"he\", \"in\", \"is\", \"it\", \"its\", \"of\", \"on\", \"that\", \"the\", \n",
    "        \"to\", \"was\", \"were\", \"will\", \"with\", \"the\", \"this\", \"but\", \"they\",\n",
    "        \"have\", \"had\", \"what\", \"when\", \"where\", \"who\", \"which\", \"why\", \"how\"\n",
    "    }\n",
    "    \n",
    "    def simple_stem(word):\n",
    "        \"\"\"Simple word stemming rules\"\"\"\n",
    "        if len(word) < 4:\n",
    "            return word\n",
    "        if word.endswith('ing'):\n",
    "            return word[:-3]\n",
    "        elif word.endswith('ed'):\n",
    "            return word[:-2]\n",
    "        elif word.endswith('s'):\n",
    "            return word[:-1]\n",
    "        return word\n",
    "    \n",
    "    words = text.split()\n",
    "    return [simple_stem(word) for word in words \n",
    "            if word not in stopwords and len(word) > 1]\n",
    "\n",
    "class TfidfVectorizer:\n",
    "    \"\"\"\n",
    "    Converts text documents to TF-IDF feature vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocabulary = set()\n",
    "        self.idf = {}\n",
    "        self.vocab_index = {}\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"\n",
    "        Builds vocabulary and computes IDF scores from documents\n",
    "        Args:\n",
    "            documents (list): List of tokenized documents\n",
    "        \"\"\"\n",
    "        # Build vocabulary\n",
    "        for doc in documents:\n",
    "            self.vocabulary.update(doc)\n",
    "        \n",
    "        self.vocab_index = {word: idx for idx, word in enumerate(sorted(self.vocabulary))}\n",
    "        \n",
    "        # Compute document frequencies\n",
    "        doc_freq = Counter()\n",
    "        for doc in documents:\n",
    "            doc_words = set(doc)\n",
    "            for word in doc_words:\n",
    "                doc_freq[word] += 1\n",
    "        \n",
    "        # Calculate IDF scores\n",
    "        num_docs = len(documents)\n",
    "        self.idf = {word: math.log((num_docs + 1)/(doc_freq[word] + 1)) + 1 \n",
    "                   for word in self.vocabulary}\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"\n",
    "        Transforms documents into TF-IDF feature vectors\n",
    "        Args:\n",
    "            documents (list): List of tokenized documents\n",
    "        Returns:\n",
    "            list: List of TF-IDF feature vectors\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        for doc in documents:\n",
    "            tf = Counter(doc)\n",
    "            doc_len = len(doc) if len(doc) > 0 else 1\n",
    "            \n",
    "            features = [0.0] * len(self.vocabulary)\n",
    "            for word in set(doc):\n",
    "                if word in self.vocab_index:\n",
    "                    idx = self.vocab_index[word]\n",
    "                    tf_val = tf[word] / doc_len\n",
    "                    features[idx] = tf_val * self.idf.get(word, 0)\n",
    "            \n",
    "            X.append(features)\n",
    "        return X\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, lambda_reg=0.1, num_epochs=100, \n",
    "                 early_stop_threshold=1e-4, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "        self.training_history = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\"\"\"\n",
    "        # Clip z to prevent overflow\n",
    "        z = min(max(z, -100), 100)\n",
    "        return 1 / (1 + math.exp(-z))\n",
    "    \n",
    "    def compute_loss(self, X, y, indices):\n",
    "        \"\"\"Compute binary cross-entropy loss with L2 regularization\"\"\"\n",
    "        loss = 0\n",
    "        for i in indices:\n",
    "            z = sum(X[i][j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "            pred = self.sigmoid(z)\n",
    "            # Add small epsilon to avoid log(0)\n",
    "            loss -= (y[i] * math.log(pred + 1e-15) + (1 - y[i]) * math.log(1 - pred + 1e-15))\n",
    "        \n",
    "        # Add L2 regularization term\n",
    "        l2_term = self.lambda_reg * sum(w * w for w in self.weights)\n",
    "        return (loss / len(indices)) + l2_term\n",
    "    \n",
    "    def compute_class_weights(self, y):\n",
    "        \"\"\"\n",
    "        Compute class weights to handle class imbalance\n",
    "        Args:\n",
    "            y (list): Labels\n",
    "        Returns:\n",
    "            dict: Class weights\n",
    "        \"\"\"\n",
    "        counts = Counter(y)\n",
    "        total = len(y)\n",
    "        weights = {\n",
    "            0: total / (2 * counts[0]) if counts[0] > 0 else 1,\n",
    "            1: total / (2 * counts[1]) if counts[1] > 0 else 1\n",
    "        }\n",
    "        print(f\"Class weights: {weights}\")\n",
    "        return weights\n",
    "    \n",
    "    def train(self, X, y, method='sgd'):\n",
    "        \"\"\"\n",
    "        Train the model using either SGD or mini-batch gradient descent\n",
    "        Args:\n",
    "            X (list): Feature vectors\n",
    "            y (list): Labels\n",
    "            method (str): 'sgd' or 'minibatch'\n",
    "        \"\"\"\n",
    "        if not self.weights:\n",
    "            # Initialize weights with small random values\n",
    "            self.weights = [random.uniform(-0.1, 0.1) for _ in range(len(X[0]))]\n",
    "        \n",
    "        # Compute class weights for balanced training\n",
    "        class_weights = self.compute_class_weights(y)\n",
    "        \n",
    "        print(f\"\\nTraining with {method.upper()}:\")\n",
    "        print(\"Epoch\\tLoss\\t\\tΔLoss\")\n",
    "        \n",
    "        n_samples = len(y)\n",
    "        prev_loss = float('inf')\n",
    "        no_improvement_count = 0\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            indices = list(range(n_samples))\n",
    "            random.shuffle(indices)\n",
    "            \n",
    "            if method == 'sgd':\n",
    "                batch_indices = [[i] for i in indices]\n",
    "            else:\n",
    "                batch_indices = [indices[i:i + self.batch_size] \n",
    "                               for i in range(0, len(indices), self.batch_size)]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            for batch in batch_indices:\n",
    "                weight_gradients = [0] * len(self.weights)\n",
    "                bias_gradient = 0\n",
    "                \n",
    "                for i in batch:\n",
    "                    # Forward pass\n",
    "                    z = sum(X[i][j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "                    pred = self.sigmoid(z)\n",
    "                    \n",
    "                    # Apply class weights to error\n",
    "                    sample_weight = class_weights[y[i]]\n",
    "                    error = sample_weight * (pred - y[i])\n",
    "                    \n",
    "                    # Accumulate gradients\n",
    "                    for j in range(len(self.weights)):\n",
    "                        weight_gradients[j] += error * X[i][j]\n",
    "                    bias_gradient += error\n",
    "                \n",
    "                # Apply updates with regularization\n",
    "                batch_size = len(batch)\n",
    "                for j in range(len(self.weights)):\n",
    "                    reg_gradient = 2 * self.lambda_reg * self.weights[j]\n",
    "                    self.weights[j] -= self.learning_rate * (weight_gradients[j]/batch_size + reg_gradient)\n",
    "                self.bias -= self.learning_rate * (bias_gradient/batch_size)\n",
    "                \n",
    "                epoch_loss += self.compute_loss(X, y, batch)\n",
    "            \n",
    "            avg_loss = epoch_loss / len(batch_indices)\n",
    "            loss_change = prev_loss - avg_loss\n",
    "            self.training_history.append(avg_loss)\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"{epoch}\\t{avg_loss:.6f}\\t{loss_change:.6f}\")\n",
    "            \n",
    "            # Early stopping with patience\n",
    "            if abs(loss_change) < self.early_stop_threshold:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= 3:  # Wait for 3 epochs of no improvement\n",
    "                    print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            else:\n",
    "                no_improvement_count = 0\n",
    "            \n",
    "            prev_loss = avg_loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            z = sum(x[j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "            prob = self.sigmoid(z)\n",
    "            predictions.append(1 if prob >= 0.5 else 0)\n",
    "        return predictions\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics\n",
    "    Returns:\n",
    "        tuple: (accuracy, precision, recall, f1)\n",
    "    \"\"\"\n",
    "    tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n",
    "    tn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 0)\n",
    "    fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n",
    "    fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n",
    "    \n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def perform_cross_validation(X, y, lambda_values, k=5):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation to find the best regularization parameter\n",
    "    Args:\n",
    "        X (list): Feature vectors\n",
    "        y (list): Labels\n",
    "        lambda_values (list): List of lambda values to try\n",
    "        k (int): Number of folds\n",
    "    Returns:\n",
    "        float: Best lambda value\n",
    "    \"\"\"\n",
    "    print(\"\\nPerforming cross-validation...\")\n",
    "    n_samples = len(y)\n",
    "    fold_size = n_samples // k\n",
    "    best_lambda = None\n",
    "    best_score = -float('inf')\n",
    "    \n",
    "    for lambda_val in lambda_values:\n",
    "        print(f\"\\nTrying lambda = {lambda_val}\")\n",
    "        scores = []\n",
    "        \n",
    "        for fold in range(k):\n",
    "            # Split data into training and validation\n",
    "            val_start = fold * fold_size\n",
    "            val_end = val_start + fold_size\n",
    "            \n",
    "            X_val = X[val_start:val_end]\n",
    "            y_val = y[val_start:val_end]\n",
    "            X_train = X[:val_start] + X[val_end:]\n",
    "            y_train = y[:val_start] + y[val_end:]\n",
    "            \n",
    "            # Train model\n",
    "            model = LogisticRegression(lambda_reg=lambda_val, num_epochs=50)\n",
    "            model.train(X_train, y_train, method='minibatch')\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_val)\n",
    "            accuracy, _, _, f1 = evaluate(y_val, y_pred)\n",
    "            scores.append(f1)\n",
    "        \n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        print(f\"Average F1-score: {avg_score:.4f}\")\n",
    "        \n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_lambda = lambda_val\n",
    "    \n",
    "    print(f\"\\nBest lambda value: {best_lambda} (F1-score: {best_score:.4f})\")\n",
    "    return best_lambda\n",
    "\n",
    "def main():\n",
    "    # Example usage with a sample input file\n",
    "    # Assuming input file format: label\\ttext\n",
    "    file_path = \"SMSSpamCollection\"  # Replace with your data file path\n",
    "    \n",
    "    # Step 1: Load and split data\n",
    "    train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = load_and_split_data(file_path)\n",
    "    \n",
    "    # Step 2: Preprocess and tokenize texts\n",
    "    print(\"\\nPreprocessing and tokenizing texts...\")\n",
    "    train_tokens = [tokenize(preprocess_text(text)) for text in train_texts]\n",
    "    val_tokens = [tokenize(preprocess_text(text)) for text in val_texts]\n",
    "    test_tokens = [tokenize(preprocess_text(text)) for text in test_texts]\n",
    "    \n",
    "    # Step 3: Create TF-IDF features\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(train_tokens)\n",
    "    \n",
    "    X_train = vectorizer.transform(train_tokens)\n",
    "    X_val = vectorizer.transform(val_tokens)\n",
    "    X_test = vectorizer.transform(test_tokens)\n",
    "    \n",
    "    # Step 4: Train and evaluate with both SGD and mini-batch\n",
    "    for method in ['sgd', 'minibatch']:\n",
    "        print(f\"\\nStep 4: Training with {method.upper()}...\")\n",
    "        model = LogisticRegression(\n",
    "            learning_rate=0.1,  # Increased learning rate\n",
    "            lambda_reg=0.01,    # Reduced regularization\n",
    "            num_epochs=35,\n",
    "            early_stop_threshold=1e-4,\n",
    "            batch_size=32\n",
    "        )\n",
    "        model.train(X_train, train_labels, method=method)\n",
    "        \n",
    "        # Evaluate on both validation and test sets\n",
    "        print(f\"\\nStep 5: Evaluating {method.upper()} model...\")\n",
    "        \n",
    "        # Validation set evaluation\n",
    "        val_pred = model.predict(X_val)\n",
    "        val_accuracy, val_precision, val_recall, val_f1 = evaluate(val_labels, val_pred)\n",
    "        print(f\"\\n{method.upper()} Validation Results:\")\n",
    "        print(f\"Accuracy:  {val_accuracy:.4f}\")\n",
    "        print(f\"Precision: {val_precision:.4f}\")\n",
    "        print(f\"Recall:    {val_recall:.4f}\")\n",
    "        print(f\"F1-score:  {val_f1:.4f}\")\n",
    "        \n",
    "        # Test set evaluation\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_accuracy, test_precision, test_recall, test_f1 = evaluate(test_labels, test_pred)\n",
    "        print(f\"\\n{method.upper()} Test Results:\")\n",
    "        print(f\"Accuracy:  {test_accuracy:.4f}\")\n",
    "        print(f\"Precision: {test_precision:.4f}\")\n",
    "        print(f\"Recall:    {test_recall:.4f}\")\n",
    "        print(f\"F1-score:  {test_f1:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part A: SMS Spam Classification\n",
      "Reading data from SMSSpamCollection...\n",
      "Total number of samples loaded: 5574\n",
      "\n",
      "Label Distribution:\n",
      "ham: 4827 (86.60%)\n",
      "spam: 747 (13.40%)\n",
      "\n",
      "Preprocessing and tokenizing texts...\n",
      "\n",
      "Step 4: Training with SGD...\n",
      "Class weights: {0: 0.5758783584292885, 1: 3.794747081712062}\n",
      "\n",
      "Training with SGD:\n",
      "Epoch\tLoss\t\tΔLoss\n",
      "0\t0.565894\tinf\n",
      "5\t0.548030\t0.004362\n",
      "10\t0.551706\t0.003164\n",
      "15\t0.549920\t0.003600\n",
      "20\t0.551242\t0.002120\n",
      "25\t0.552730\t-0.001959\n",
      "30\t0.554134\t-0.004919\n",
      "\n",
      "Step 5: Evaluating SGD model...\n",
      "\n",
      "SGD Validation Results:\n",
      "Accuracy:  0.9187\n",
      "Precision: 0.6524\n",
      "Recall:    0.9068\n",
      "F1-score:  0.7589\n",
      "\n",
      "SGD Test Results:\n",
      "Accuracy:  0.9283\n",
      "Precision: 0.6821\n",
      "Recall:    0.8957\n",
      "F1-score:  0.7744\n",
      "\n",
      "Step 4: Training with MINIBATCH...\n",
      "Class weights: {0: 0.5758783584292885, 1: 3.794747081712062}\n",
      "\n",
      "Training with MINIBATCH:\n",
      "Epoch\tLoss\t\tΔLoss\n",
      "0\t0.826326\tinf\n",
      "5\t0.604667\t0.011946\n",
      "10\t0.589657\t0.001560\n",
      "15\t0.588511\t-0.005011\n",
      "20\t0.580150\t0.010970\n",
      "25\t0.592079\t-0.009504\n",
      "30\t0.588796\t-0.002602\n",
      "\n",
      "Step 5: Evaluating MINIBATCH model...\n",
      "\n",
      "MINIBATCH Validation Results:\n",
      "Accuracy:  0.9414\n",
      "Precision: 0.7518\n",
      "Recall:    0.8729\n",
      "F1-score:  0.8078\n",
      "\n",
      "MINIBATCH Test Results:\n",
      "Accuracy:  0.9498\n",
      "Precision: 0.7744\n",
      "Recall:    0.8957\n",
      "F1-score:  0.8306\n",
      "\n",
      "Part B: Author Attribution\n",
      "\n",
      "Reading books data from books.txt...\n",
      "Total number of book samples loaded: 19536\n",
      "\n",
      "Author Distribution:\n",
      "Jane Austen: 11054 (56.58%)\n",
      "Arthur Conan Doyle: 2538 (12.99%)\n",
      "Fyodor Dostoyevsky: 5944 (30.43%)\n",
      "\n",
      "Preprocessing and tokenizing book texts...\n",
      "\n",
      "Training multi-class model for 3 authors...\n",
      "\n",
      "Training multi-class model with MINIBATCH:\n",
      "Epoch\tLoss\t\tΔLoss\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 622\u001b[0m\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMacro F1-score:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 622\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 598\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining multi-class model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m authors...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    588\u001b[0m multi_model \u001b[38;5;241m=\u001b[39m MultiClassLogisticRegression(\n\u001b[0;32m    589\u001b[0m     n_classes\u001b[38;5;241m=\u001b[39mn_classes,\n\u001b[0;32m    590\u001b[0m     n_features\u001b[38;5;241m=\u001b[39mn_features,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    595\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\n\u001b[0;32m    596\u001b[0m )\n\u001b[1;32m--> 598\u001b[0m \u001b[43mmulti_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_book_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbook_train_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;66;03m# Evaluate multi-class model\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating multi-class model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 429\u001b[0m, in \u001b[0;36mMultiClassLogisticRegression.train\u001b[1;34m(self, X, y, method)\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[c][j] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m (weight_gradients[c][j]\u001b[38;5;241m/\u001b[39mbatch_size \u001b[38;5;241m+\u001b[39m reg_gradient)\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[c] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m (bias_gradients[c]\u001b[38;5;241m/\u001b[39mbatch_size)\n\u001b[1;32m--> 429\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    431\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_indices)\n\u001b[0;32m    432\u001b[0m loss_change \u001b[38;5;241m=\u001b[39m prev_loss \u001b[38;5;241m-\u001b[39m avg_loss\n",
      "Cell \u001b[1;32mIn[2], line 369\u001b[0m, in \u001b[0;36mMultiClassLogisticRegression.compute_loss\u001b[1;34m(self, X, y, indices)\u001b[0m\n\u001b[0;32m    366\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;66;03m# Compute scores for each class\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(X[i][j] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[c][j] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X[i]))) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[c]\n\u001b[0;32m    370\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes)]\n\u001b[0;32m    371\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(scores)\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;66;03m# Add small epsilon to avoid log(0)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 369\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    366\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;66;03m# Compute scores for each class\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[c]\n\u001b[0;32m    370\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes)]\n\u001b[0;32m    371\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(scores)\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;66;03m# Add small epsilon to avoid log(0)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 369\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    366\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;66;03m# Compute scores for each class\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(X[i][j] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[c][j] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X[i]))) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[c]\n\u001b[0;32m    370\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes)]\n\u001b[0;32m    371\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(scores)\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;66;03m# Add small epsilon to avoid log(0)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "def load_and_split_data(file_path, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Loads text data from a file and splits it into training, validation, and test sets.\n",
    "    Args:\n",
    "        file_path (str): Path to the data file\n",
    "        train_ratio (float): Proportion of data for training\n",
    "        val_ratio (float): Proportion of data for validation\n",
    "    Returns:\n",
    "        tuple: Contains train, validation, and test data and labels\n",
    "    \"\"\"\n",
    "    print(f\"Reading data from {file_path}...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"Total number of samples loaded: {len(lines)}\")\n",
    "    \n",
    "    # Count label distribution\n",
    "    label_counts = Counter(line.split('\\t')[0] for line in lines)\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"{label}: {count} ({count/len(lines)*100:.2f}%)\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    # Calculate split points\n",
    "    train_end = int(len(lines) * train_ratio)\n",
    "    val_end = int(len(lines) * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Split data\n",
    "    train_data = lines[:train_end]\n",
    "    val_data = lines[train_end:val_end]\n",
    "    test_data = lines[val_end:]\n",
    "    \n",
    "    def split_data(data):\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in data:\n",
    "            if '\\t' in line:\n",
    "                label, text = line.strip().split('\\t', 1)\n",
    "                # Convert 'spam' to 1 and 'ham' to 0\n",
    "                label_int = 1 if label.lower() == 'spam' else 0\n",
    "                labels.append(label_int)\n",
    "                sentences.append(text)\n",
    "        return sentences, labels\n",
    "    \n",
    "    return split_data(train_data) + split_data(val_data) + split_data(test_data)\n",
    "\n",
    "def load_books_data(file_path, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Loads book text data and splits it into training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    print(f\"\\nReading books data from {file_path}...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"Total number of book samples loaded: {len(lines)}\")\n",
    "    \n",
    "    # Count author distribution\n",
    "    label_counts = Counter(line.split('\\t')[0] for line in lines)\n",
    "    print(\"\\nAuthor Distribution:\")\n",
    "    for author, count in label_counts.items():\n",
    "        print(f\"{author}: {count} ({count/len(lines)*100:.2f}%)\")\n",
    "    \n",
    "    # Create author to index mapping\n",
    "    authors = sorted(list(label_counts.keys()))\n",
    "    author_to_idx = {author: idx for idx, author in enumerate(authors)}\n",
    "    \n",
    "    # Shuffle data\n",
    "    random.seed(42)\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    # Calculate split points\n",
    "    train_end = int(len(lines) * train_ratio)\n",
    "    val_end = int(len(lines) * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Split data\n",
    "    train_data = lines[:train_end]\n",
    "    val_data = lines[train_end:val_end]\n",
    "    test_data = lines[val_end:]\n",
    "    \n",
    "    def split_books_data(data):\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in data:\n",
    "            if '\\t' in line:\n",
    "                author, text = line.strip().split('\\t', 1)\n",
    "                labels.append(author_to_idx[author])\n",
    "                sentences.append(text)\n",
    "        return sentences, labels\n",
    "    \n",
    "    return (split_books_data(train_data) + split_books_data(val_data) + \n",
    "            split_books_data(test_data) + (authors,))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing special characters and converting to lowercase.\n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = ' '.join(word for word in text.split() \n",
    "                   if not word.startswith(('http:', 'https:', 'www.')))\n",
    "    \n",
    "    # Keep only letters and spaces\n",
    "    cleaned_text = \"\"\n",
    "    for char in text:\n",
    "        if char.isalpha() or char.isspace():\n",
    "            cleaned_text += char\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Converts text into tokens, removes stopwords, and applies stemming.\n",
    "    Args:\n",
    "        text (str): Input text to tokenize\n",
    "    Returns:\n",
    "        list: List of processed tokens\n",
    "    \"\"\"\n",
    "    stopwords = {\n",
    "        \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\", \n",
    "        \"has\", \"he\", \"in\", \"is\", \"it\", \"its\", \"of\", \"on\", \"that\", \"the\", \n",
    "        \"to\", \"was\", \"were\", \"will\", \"with\", \"the\", \"this\", \"but\", \"they\",\n",
    "        \"have\", \"had\", \"what\", \"when\", \"where\", \"who\", \"which\", \"why\", \"how\"\n",
    "    }\n",
    "    \n",
    "    def simple_stem(word):\n",
    "        \"\"\"Simple word stemming rules\"\"\"\n",
    "        if len(word) < 4:\n",
    "            return word\n",
    "        if word.endswith('ing'):\n",
    "            return word[:-3]\n",
    "        elif word.endswith('ed'):\n",
    "            return word[:-2]\n",
    "        elif word.endswith('s'):\n",
    "            return word[:-1]\n",
    "        return word\n",
    "    \n",
    "    words = text.split()\n",
    "    return [simple_stem(word) for word in words \n",
    "            if word not in stopwords and len(word) > 1]\n",
    "\n",
    "class TfidfVectorizer:\n",
    "    \"\"\"\n",
    "    Converts text documents to TF-IDF feature vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocabulary = set()\n",
    "        self.idf = {}\n",
    "        self.vocab_index = {}\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"\n",
    "        Builds vocabulary and computes IDF scores from documents\n",
    "        Args:\n",
    "            documents (list): List of tokenized documents\n",
    "        \"\"\"\n",
    "        # Build vocabulary\n",
    "        for doc in documents:\n",
    "            self.vocabulary.update(doc)\n",
    "        \n",
    "        self.vocab_index = {word: idx for idx, word in enumerate(sorted(self.vocabulary))}\n",
    "        \n",
    "        # Compute document frequencies\n",
    "        doc_freq = Counter()\n",
    "        for doc in documents:\n",
    "            doc_words = set(doc)\n",
    "            for word in doc_words:\n",
    "                doc_freq[word] += 1\n",
    "        \n",
    "        # Calculate IDF scores\n",
    "        num_docs = len(documents)\n",
    "        self.idf = {word: math.log((num_docs + 1)/(doc_freq[word] + 1)) + 1 \n",
    "                   for word in self.vocabulary}\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"\n",
    "        Transforms documents into TF-IDF feature vectors\n",
    "        Args:\n",
    "            documents (list): List of tokenized documents\n",
    "        Returns:\n",
    "            list: List of TF-IDF feature vectors\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        for doc in documents:\n",
    "            tf = Counter(doc)\n",
    "            doc_len = len(doc) if len(doc) > 0 else 1\n",
    "            \n",
    "            features = [0.0] * len(self.vocabulary)\n",
    "            for word in set(doc):\n",
    "                if word in self.vocab_index:\n",
    "                    idx = self.vocab_index[word]\n",
    "                    tf_val = tf[word] / doc_len\n",
    "                    features[idx] = tf_val * self.idf.get(word, 0)\n",
    "            \n",
    "            X.append(features)\n",
    "        return X\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, lambda_reg=0.1, num_epochs=100, \n",
    "                 early_stop_threshold=1e-4, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "        self.training_history = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\"\"\"\n",
    "        # Clip z to prevent overflow\n",
    "        z = min(max(z, -100), 100)\n",
    "        return 1 / (1 + math.exp(-z))\n",
    "    \n",
    "    def compute_loss(self, X, y, indices):\n",
    "        \"\"\"Compute binary cross-entropy loss with L2 regularization\"\"\"\n",
    "        loss = 0\n",
    "        for i in indices:\n",
    "            z = sum(X[i][j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "            pred = self.sigmoid(z)\n",
    "            # Add small epsilon to avoid log(0)\n",
    "            loss -= (y[i] * math.log(pred + 1e-15) + (1 - y[i]) * math.log(1 - pred + 1e-15))\n",
    "        \n",
    "        # Add L2 regularization term\n",
    "        l2_term = self.lambda_reg * sum(w * w for w in self.weights)\n",
    "        return (loss / len(indices)) + l2_term\n",
    "    \n",
    "    def compute_class_weights(self, y):\n",
    "        \"\"\"\n",
    "        Compute class weights to handle class imbalance\n",
    "        Args:\n",
    "            y (list): Labels\n",
    "        Returns:\n",
    "            dict: Class weights\n",
    "        \"\"\"\n",
    "        counts = Counter(y)\n",
    "        total = len(y)\n",
    "        weights = {\n",
    "            0: total / (2 * counts[0]) if counts[0] > 0 else 1,\n",
    "            1: total / (2 * counts[1]) if counts[1] > 0 else 1\n",
    "        }\n",
    "        print(f\"Class weights: {weights}\")\n",
    "        return weights\n",
    "    \n",
    "    def train(self, X, y, method='sgd'):\n",
    "        \"\"\"\n",
    "        Train the model using either SGD or mini-batch gradient descent\n",
    "        Args:\n",
    "            X (list): Feature vectors\n",
    "            y (list): Labels\n",
    "            method (str): 'sgd' or 'minibatch'\n",
    "        \"\"\"\n",
    "        if not self.weights:\n",
    "            # Initialize weights with small random values\n",
    "            self.weights = [random.uniform(-0.1, 0.1) for _ in range(len(X[0]))]\n",
    "        \n",
    "        # Compute class weights for balanced training\n",
    "        class_weights = self.compute_class_weights(y)\n",
    "        \n",
    "        print(f\"\\nTraining with {method.upper()}:\")\n",
    "        print(\"Epoch\\tLoss\\t\\tΔLoss\")\n",
    "        \n",
    "        n_samples = len(y)\n",
    "        prev_loss = float('inf')\n",
    "        no_improvement_count = 0\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            indices = list(range(n_samples))\n",
    "            random.shuffle(indices)\n",
    "            \n",
    "            if method == 'sgd':\n",
    "                batch_indices = [[i] for i in indices]\n",
    "            else:\n",
    "                batch_indices = [indices[i:i + self.batch_size] \n",
    "                               for i in range(0, len(indices), self.batch_size)]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            for batch in batch_indices:\n",
    "                weight_gradients = [0] * len(self.weights)\n",
    "                bias_gradient = 0\n",
    "                \n",
    "                for i in batch:\n",
    "                    # Forward pass\n",
    "                    z = sum(X[i][j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "                    pred = self.sigmoid(z)\n",
    "                    \n",
    "                    # Apply class weights to error\n",
    "                    sample_weight = class_weights[y[i]]\n",
    "                    error = sample_weight * (pred - y[i])\n",
    "                    \n",
    "                    # Accumulate gradients\n",
    "                    for j in range(len(self.weights)):\n",
    "                        weight_gradients[j] += error * X[i][j]\n",
    "                    bias_gradient += error\n",
    "                \n",
    "                # Apply updates with regularization\n",
    "                batch_size = len(batch)\n",
    "                for j in range(len(self.weights)):\n",
    "                    reg_gradient = 2 * self.lambda_reg * self.weights[j]\n",
    "                    self.weights[j] -= self.learning_rate * (weight_gradients[j]/batch_size + reg_gradient)\n",
    "                self.bias -= self.learning_rate * (bias_gradient/batch_size)\n",
    "                \n",
    "                epoch_loss += self.compute_loss(X, y, batch)\n",
    "            \n",
    "            avg_loss = epoch_loss / len(batch_indices)\n",
    "            loss_change = prev_loss - avg_loss\n",
    "            self.training_history.append(avg_loss)\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"{epoch}\\t{avg_loss:.6f}\\t{loss_change:.6f}\")\n",
    "            \n",
    "            # Early stopping with patience\n",
    "            if abs(loss_change) < self.early_stop_threshold:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= 3:  # Wait for 3 epochs of no improvement\n",
    "                    print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            else:\n",
    "                no_improvement_count = 0\n",
    "            \n",
    "            prev_loss = avg_loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            z = sum(x[j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "            prob = self.sigmoid(z)\n",
    "            predictions.append(1 if prob >= 0.5 else 0)\n",
    "        return predictions\n",
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, n_classes, n_features, learning_rate=0.01, lambda_reg=0.1, \n",
    "                 num_epochs=100, early_stop_threshold=1e-4, batch_size=32):\n",
    "        self.n_classes = n_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "        self.batch_size = batch_size\n",
    "        # Initialize weights for each class\n",
    "        self.weights = [[random.uniform(-0.1, 0.1) for _ in range(n_features)] \n",
    "                       for _ in range(n_classes)]\n",
    "        self.biases = [0] * n_classes\n",
    "        self.training_history = []\n",
    "    \n",
    "    def softmax(self, scores):\n",
    "        \"\"\"Compute softmax probabilities\"\"\"\n",
    "        exp_scores = [math.exp(min(max(s, -100), 100)) for s in scores]\n",
    "        total = sum(exp_scores)\n",
    "        return [s / total for s in exp_scores]\n",
    "    \n",
    "    def compute_loss(self, X, y, indices):\n",
    "        \"\"\"Compute categorical cross-entropy loss with L2 regularization\"\"\"\n",
    "        loss = 0\n",
    "        for i in indices:\n",
    "            # Compute scores for each class\n",
    "            scores = [sum(X[i][j] * self.weights[c][j] for j in range(len(X[i]))) + self.biases[c]\n",
    "                     for c in range(self.n_classes)]\n",
    "            probs = self.softmax(scores)\n",
    "            # Add small epsilon to avoid log(0)\n",
    "            loss -= math.log(probs[y[i]] + 1e-15)\n",
    "        \n",
    "        # Add L2 regularization term\n",
    "        l2_term = self.lambda_reg * sum(sum(w * w for w in class_weights) \n",
    "                                      for class_weights in self.weights)\n",
    "        return (loss / len(indices)) + l2_term\n",
    "    \n",
    "    def train(self, X, y, method='minibatch'):\n",
    "        \"\"\"Train the model using mini-batch gradient descent\"\"\"\n",
    "        print(f\"\\nTraining multi-class model with {method.upper()}:\")\n",
    "        print(\"Epoch\\tLoss\\t\\tΔLoss\")\n",
    "        \n",
    "        n_samples = len(y)\n",
    "        prev_loss = float('inf')\n",
    "        no_improvement_count = 0\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            indices = list(range(n_samples))\n",
    "            random.shuffle(indices)\n",
    "            \n",
    "            if method == 'sgd':\n",
    "                batch_indices = [[i] for i in indices]\n",
    "            else:\n",
    "                batch_indices = [indices[i:i + self.batch_size] \n",
    "                               for i in range(0, len(indices), self.batch_size)]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            for batch in batch_indices:\n",
    "                # Initialize gradients\n",
    "                weight_gradients = [[0] * len(self.weights[0]) for _ in range(self.n_classes)]\n",
    "                bias_gradients = [0] * self.n_classes\n",
    "                \n",
    "                for i in batch:\n",
    "                    # Forward pass\n",
    "                    scores = [sum(X[i][j] * self.weights[c][j] for j in range(len(X[i]))) + self.biases[c]\n",
    "                            for c in range(self.n_classes)]\n",
    "                    probs = self.softmax(scores)\n",
    "                    \n",
    "                    # Compute gradients\n",
    "                    for c in range(self.n_classes):\n",
    "                        error = probs[c]\n",
    "                        if c == y[i]:\n",
    "                            error -= 1\n",
    "                            \n",
    "                        for j in range(len(X[i])):\n",
    "                            weight_gradients[c][j] += error * X[i][j]\n",
    "                        bias_gradients[c] += error\n",
    "                \n",
    "                # Apply updates with regularization\n",
    "                batch_size = len(batch)\n",
    "                for c in range(self.n_classes):\n",
    "                    for j in range(len(self.weights[c])):\n",
    "                        reg_gradient = 2 * self.lambda_reg * self.weights[c][j]\n",
    "                        self.weights[c][j] -= self.learning_rate * (weight_gradients[c][j]/batch_size + reg_gradient)\n",
    "                    self.biases[c] -= self.learning_rate * (bias_gradients[c]/batch_size)\n",
    "                \n",
    "                epoch_loss += self.compute_loss(X, y, batch)\n",
    "            \n",
    "            avg_loss = epoch_loss / len(batch_indices)\n",
    "            loss_change = prev_loss - avg_loss\n",
    "            self.training_history.append(avg_loss)\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"{epoch}\\t{avg_loss:.6f}\\t{loss_change:.6f}\")\n",
    "            \n",
    "            if abs(loss_change) < self.early_stop_threshold:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= 3:\n",
    "                    print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            else:\n",
    "                no_improvement_count = 0\n",
    "            \n",
    "            prev_loss = avg_loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            scores = [sum(x[j] * self.weights[c][j] for j in range(len(x))) + self.biases[c]\n",
    "                     for c in range(self.n_classes)]\n",
    "            probs = self.softmax(scores)\n",
    "            predictions.append(max(range(len(probs)), key=lambda i: probs[i]))\n",
    "        return predictions\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics\n",
    "    Returns:\n",
    "        tuple: (accuracy, precision, recall, f1)\n",
    "    \"\"\"\n",
    "    tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n",
    "    tn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 0)\n",
    "    fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n",
    "    fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n",
    "    \n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def evaluate_multiclass(y_true, y_pred, n_classes):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for multi-class classification\n",
    "    Returns:\n",
    "        tuple: (accuracy, macro_precision, macro_recall, macro_f1)\n",
    "    \"\"\"\n",
    "    # Compute per-class metrics\n",
    "    class_metrics = []\n",
    "    for c in range(n_classes):\n",
    "        tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == c and yp == c)\n",
    "        fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt != c and yp == c)\n",
    "        fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == c and yp != c)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        class_metrics.append((precision, recall, f1))\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(1 for yt, yp in zip(y_true, y_pred) if yt == yp) / len(y_true)\n",
    "    \n",
    "    # Calculate macro-averaged metrics\n",
    "    macro_precision = sum(m[0] for m in class_metrics) / n_classes\n",
    "    macro_recall = sum(m[1] for m in class_metrics) / n_classes\n",
    "    macro_f1 = sum(m[2] for m in class_metrics) / n_classes\n",
    "    \n",
    "    return accuracy, macro_precision, macro_recall, macro_f1\n",
    "\n",
    "def main():\n",
    "    # Part A: Binary Classification (SMS Spam)\n",
    "    print(\"Part A: SMS Spam Classification\")\n",
    "    file_path = \"SMSSpamCollection\"\n",
    "    \n",
    "    # Step 1: Load and split data\n",
    "    train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = load_and_split_data(file_path)\n",
    "    \n",
    "    # Step 2: Preprocess and tokenize texts\n",
    "    print(\"\\nPreprocessing and tokenizing texts...\")\n",
    "    train_tokens = [tokenize(preprocess_text(text)) for text in train_texts]\n",
    "    val_tokens = [tokenize(preprocess_text(text)) for text in val_texts]\n",
    "    test_tokens = [tokenize(preprocess_text(text)) for text in test_texts]\n",
    "    \n",
    "    # Step 3: Create TF-IDF features\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(train_tokens)\n",
    "    \n",
    "    X_train = vectorizer.transform(train_tokens)\n",
    "    X_val = vectorizer.transform(val_tokens)\n",
    "    X_test = vectorizer.transform(test_tokens)\n",
    "    \n",
    "    # Step 4: Train and evaluate with both SGD and mini-batch\n",
    "    for method in ['sgd', 'minibatch']:\n",
    "        print(f\"\\nStep 4: Training with {method.upper()}...\")\n",
    "        model = LogisticRegression(\n",
    "            learning_rate=0.1,\n",
    "            lambda_reg=0.01,\n",
    "            num_epochs=35,\n",
    "            early_stop_threshold=1e-4,\n",
    "            batch_size=32\n",
    "        )\n",
    "        model.train(X_train, train_labels, method=method)\n",
    "        \n",
    "        # Evaluate on both validation and test sets\n",
    "        print(f\"\\nStep 5: Evaluating {method.upper()} model...\")\n",
    "        \n",
    "        # Validation set evaluation\n",
    "        val_pred = model.predict(X_val)\n",
    "        val_accuracy, val_precision, val_recall, val_f1 = evaluate(val_labels, val_pred)\n",
    "        print(f\"\\n{method.upper()} Validation Results:\")\n",
    "        print(f\"Accuracy:  {val_accuracy:.4f}\")\n",
    "        print(f\"Precision: {val_precision:.4f}\")\n",
    "        print(f\"Recall:    {val_recall:.4f}\")\n",
    "        print(f\"F1-score:  {val_f1:.4f}\")\n",
    "        \n",
    "        # Test set evaluation\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_accuracy, test_precision, test_recall, test_f1 = evaluate(test_labels, test_pred)\n",
    "        print(f\"\\n{method.upper()} Test Results:\")\n",
    "        print(f\"Accuracy:  {test_accuracy:.4f}\")\n",
    "        print(f\"Precision: {test_precision:.4f}\")\n",
    "        print(f\"Recall:    {test_recall:.4f}\")\n",
    "        print(f\"F1-score:  {test_f1:.4f}\")\n",
    "    \n",
    "    # Part B: Multi-class Classification (Author Attribution)\n",
    "    print(\"\\nPart B: Author Attribution\")\n",
    "    books_file = \"books.txt\"\n",
    "    \n",
    "    # Load and split books data\n",
    "    (book_train_texts, book_train_labels, \n",
    "     book_val_texts, book_val_labels,\n",
    "     book_test_texts, book_test_labels,\n",
    "     authors) = load_books_data(books_file)\n",
    "    \n",
    "    # Preprocess and tokenize book texts\n",
    "    print(\"\\nPreprocessing and tokenizing book texts...\")\n",
    "    book_train_tokens = [tokenize(preprocess_text(text)) for text in book_train_texts]\n",
    "    book_val_tokens = [tokenize(preprocess_text(text)) for text in book_val_texts]\n",
    "    book_test_tokens = [tokenize(preprocess_text(text)) for text in book_test_texts]\n",
    "    \n",
    "    # Create TF-IDF features for books\n",
    "    book_vectorizer = TfidfVectorizer()\n",
    "    book_vectorizer.fit(book_train_tokens)\n",
    "    \n",
    "    X_book_train = book_vectorizer.transform(book_train_tokens)\n",
    "    X_book_val = book_vectorizer.transform(book_val_tokens)\n",
    "    X_book_test = book_vectorizer.transform(book_test_tokens)\n",
    "    \n",
    "    # Train multi-class model\n",
    "    n_classes = len(authors)\n",
    "    n_features = len(book_vectorizer.vocabulary)\n",
    "    \n",
    "    print(f\"\\nTraining multi-class model for {n_classes} authors...\")\n",
    "    multi_model = MultiClassLogisticRegression(\n",
    "        n_classes=n_classes,\n",
    "        n_features=n_features,\n",
    "        learning_rate=0.1,\n",
    "        lambda_reg=0.01,\n",
    "        num_epochs=35,\n",
    "        early_stop_threshold=1e-4,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    multi_model.train(X_book_train, book_train_labels)\n",
    "    \n",
    "    # Evaluate multi-class model\n",
    "    print(\"\\nEvaluating multi-class model...\")\n",
    "    \n",
    "    # Validation set evaluation\n",
    "    val_pred = multi_model.predict(X_book_val)\n",
    "    val_metrics = evaluate_multiclass(book_val_labels, val_pred, n_classes)\n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(f\"Accuracy:        {val_metrics[0]:.4f}\")\n",
    "    print(f\"Macro Precision: {val_metrics[1]:.4f}\")\n",
    "    print(f\"Macro Recall:    {val_metrics[2]:.4f}\")\n",
    "    print(f\"Macro F1-score:  {val_metrics[3]:.4f}\")\n",
    "    \n",
    "    # Test set evaluation\n",
    "    test_pred = multi_model.predict(X_book_test)\n",
    "    test_metrics = evaluate_multiclass(book_test_labels, test_pred, n_classes)\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Accuracy:        {test_metrics[0]:.4f}\")\n",
    "    print(f\"Macro Precision: {test_metrics[1]:.4f}\")\n",
    "    print(f\"Macro Recall:    {test_metrics[2]:.4f}\")\n",
    "    print(f\"Macro F1-score:  {test_metrics[3]:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
