{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "Hide-cell"
    ]
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n\u001b[1;32m--> 988\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "def load_and_split_data(file_path, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Loads text data from a file and splits it into training, validation, and test sets.\n",
    "    Args:\n",
    "        file_path (str): Path to the data file\n",
    "        train_ratio (float): Proportion of data for training\n",
    "        val_ratio (float): Proportion of data for validation\n",
    "    Returns:\n",
    "        tuple: Contains train, validation, and test data and labels\n",
    "    \"\"\"\n",
    "    print(f\"Reading data from {file_path}...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"Total number of samples loaded: {len(lines)}\")\n",
    "    \n",
    "    # Count label distribution\n",
    "    label_counts = Counter(line.split('\\t')[0] for line in lines)\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"{label}: {count} ({count/len(lines)*100:.2f}%)\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    # Calculate split points\n",
    "    train_end = int(len(lines) * train_ratio)\n",
    "    val_end = int(len(lines) * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Split data\n",
    "    train_data = lines[:train_end]\n",
    "    val_data = lines[train_end:val_end]\n",
    "    test_data = lines[val_end:]\n",
    "    \n",
    "    def split_data(data):\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in data:\n",
    "            if '\\t' in line:\n",
    "                label, text = line.strip().split('\\t', 1)\n",
    "                # Convert 'spam' to 1 and 'ham' to 0\n",
    "                label_int = 1 if label.lower() == 'spam' else 0\n",
    "                labels.append(label_int)\n",
    "                sentences.append(text)\n",
    "        return sentences, labels\n",
    "    \n",
    "    return split_data(train_data) + split_data(val_data) + split_data(test_data)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing special characters and converting to lowercase.\n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = ' '.join(word for word in text.split() \n",
    "                   if not word.startswith(('http:', 'https:', 'www.')))\n",
    "    \n",
    "    # Keep only letters and spaces\n",
    "    cleaned_text = \"\"\n",
    "    for char in text:\n",
    "        if char.isalpha() or char.isspace():\n",
    "            cleaned_text += char\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Converts text into tokens, removes stopwords, and applies stemming.\n",
    "    Args:\n",
    "        text (str): Input text to tokenize\n",
    "    Returns:\n",
    "        list: List of processed tokens\n",
    "    \"\"\"\n",
    "    stopwords = {\n",
    "        \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\", \n",
    "        \"has\", \"he\", \"in\", \"is\", \"it\", \"its\", \"of\", \"on\", \"that\", \"the\", \n",
    "        \"to\", \"was\", \"were\", \"will\", \"with\", \"the\", \"this\", \"but\", \"they\",\n",
    "        \"have\", \"had\", \"what\", \"when\", \"where\", \"who\", \"which\", \"why\", \"how\"\n",
    "    }\n",
    "    \n",
    "    def simple_stem(word):\n",
    "        \"\"\"Simple word stemming rules\"\"\"\n",
    "        if len(word) < 4:\n",
    "            return word\n",
    "        if word.endswith('ing'):\n",
    "            return word[:-3]\n",
    "        elif word.endswith('ed'):\n",
    "            return word[:-2]\n",
    "        elif word.endswith('s'):\n",
    "            return word[:-1]\n",
    "        return word\n",
    "    \n",
    "    words = text.split()\n",
    "    return [simple_stem(word) for word in words \n",
    "            if word not in stopwords and len(word) > 1]\n",
    "\n",
    "class TfidfVectorizer:\n",
    "    \"\"\"\n",
    "    Converts text documents to TF-IDF feature vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocabulary = set()\n",
    "        self.idf = {}\n",
    "        self.vocab_index = {}\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"\n",
    "        Builds vocabulary and computes IDF scores from documents\n",
    "        Args:\n",
    "            documents (list): List of tokenized documents\n",
    "        \"\"\"\n",
    "        # Build vocabulary\n",
    "        for doc in documents:\n",
    "            self.vocabulary.update(doc)\n",
    "        \n",
    "        self.vocab_index = {word: idx for idx, word in enumerate(sorted(self.vocabulary))}\n",
    "        \n",
    "        # Compute document frequencies\n",
    "        doc_freq = Counter()\n",
    "        for doc in documents:\n",
    "            doc_words = set(doc)\n",
    "            for word in doc_words:\n",
    "                doc_freq[word] += 1\n",
    "        \n",
    "        # Calculate IDF scores\n",
    "        num_docs = len(documents)\n",
    "        self.idf = {word: math.log((num_docs + 1)/(doc_freq[word] + 1)) + 1 \n",
    "                   for word in self.vocabulary}\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"\n",
    "        Transforms documents into TF-IDF feature vectors\n",
    "        Args:\n",
    "            documents (list): List of tokenized documents\n",
    "        Returns:\n",
    "            list: List of TF-IDF feature vectors\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        for doc in documents:\n",
    "            tf = Counter(doc)\n",
    "            doc_len = len(doc) if len(doc) > 0 else 1\n",
    "            \n",
    "            features = [0.0] * len(self.vocabulary)\n",
    "            for word in set(doc):\n",
    "                if word in self.vocab_index:\n",
    "                    idx = self.vocab_index[word]\n",
    "                    tf_val = tf[word] / doc_len\n",
    "                    features[idx] = tf_val * self.idf.get(word, 0)\n",
    "            \n",
    "            X.append(features)\n",
    "        return X\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, lambda_reg=0.1, num_epochs=100, \n",
    "                 early_stop_threshold=1e-4, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "        self.training_history = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\"\"\"\n",
    "        # Clip z to prevent overflow\n",
    "        z = min(max(z, -100), 100)\n",
    "        return 1 / (1 + math.exp(-z))\n",
    "    \n",
    "    def compute_loss(self, X, y, indices):\n",
    "        \"\"\"Compute binary cross-entropy loss with L2 regularization\"\"\"\n",
    "        loss = 0\n",
    "        for i in indices:\n",
    "            z = sum(X[i][j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "            pred = self.sigmoid(z)\n",
    "            # Add small epsilon to avoid log(0)\n",
    "            loss -= (y[i] * math.log(pred + 1e-15) + (1 - y[i]) * math.log(1 - pred + 1e-15))\n",
    "        \n",
    "        # Add L2 regularization term\n",
    "        l2_term = self.lambda_reg * sum(w * w for w in self.weights)\n",
    "        return (loss / len(indices)) + l2_term\n",
    "    \n",
    "    def compute_class_weights(self, y):\n",
    "        \"\"\"\n",
    "        Compute class weights to handle class imbalance\n",
    "        Args:\n",
    "            y (list): Labels\n",
    "        Returns:\n",
    "            dict: Class weights\n",
    "        \"\"\"\n",
    "        counts = Counter(y)\n",
    "        total = len(y)\n",
    "        weights = {\n",
    "            0: total / (2 * counts[0]) if counts[0] > 0 else 1,\n",
    "            1: total / (2 * counts[1]) if counts[1] > 0 else 1\n",
    "        }\n",
    "        print(f\"Class weights: {weights}\")\n",
    "        return weights\n",
    "    \n",
    "    def train(self, X, y, method='sgd'):\n",
    "        \"\"\"\n",
    "        Train the model using either SGD or mini-batch gradient descent\n",
    "        Args:\n",
    "            X (list): Feature vectors\n",
    "            y (list): Labels\n",
    "            method (str): 'sgd' or 'minibatch'\n",
    "        \"\"\"\n",
    "        if not self.weights:\n",
    "            # Initialize weights with small random values\n",
    "            self.weights = [random.uniform(-0.1, 0.1) for _ in range(len(X[0]))]\n",
    "        \n",
    "        # Compute class weights for balanced training\n",
    "        class_weights = self.compute_class_weights(y)\n",
    "        \n",
    "        print(f\"\\nTraining with {method.upper()}:\")\n",
    "        print(\"Epoch\\tLoss\\t\\tΔLoss\")\n",
    "        \n",
    "        n_samples = len(y)\n",
    "        prev_loss = float('inf')\n",
    "        no_improvement_count = 0\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            indices = list(range(n_samples))\n",
    "            random.shuffle(indices)\n",
    "            \n",
    "            if method == 'sgd':\n",
    "                batch_indices = [[i] for i in indices]\n",
    "            else:\n",
    "                batch_indices = [indices[i:i + self.batch_size] \n",
    "                               for i in range(0, len(indices), self.batch_size)]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            for batch in batch_indices:\n",
    "                weight_gradients = [0] * len(self.weights)\n",
    "                bias_gradient = 0\n",
    "                \n",
    "                for i in batch:\n",
    "                    # Forward pass\n",
    "                    z = sum(X[i][j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "                    pred = self.sigmoid(z)\n",
    "                    \n",
    "                    # Apply class weights to error\n",
    "                    sample_weight = class_weights[y[i]]\n",
    "                    error = sample_weight * (pred - y[i])\n",
    "                    \n",
    "                    # Accumulate gradients\n",
    "                    for j in range(len(self.weights)):\n",
    "                        weight_gradients[j] += error * X[i][j]\n",
    "                    bias_gradient += error\n",
    "                \n",
    "                # Apply updates with regularization\n",
    "                batch_size = len(batch)\n",
    "                for j in range(len(self.weights)):\n",
    "                    reg_gradient = 2 * self.lambda_reg * self.weights[j]\n",
    "                    self.weights[j] -= self.learning_rate * (weight_gradients[j]/batch_size + reg_gradient)\n",
    "                self.bias -= self.learning_rate * (bias_gradient/batch_size)\n",
    "                \n",
    "                epoch_loss += self.compute_loss(X, y, batch)\n",
    "            \n",
    "            avg_loss = epoch_loss / len(batch_indices)\n",
    "            loss_change = prev_loss - avg_loss\n",
    "            self.training_history.append(avg_loss)\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"{epoch}\\t{avg_loss:.6f}\\t{loss_change:.6f}\")\n",
    "            \n",
    "            # Early stopping with patience\n",
    "            if abs(loss_change) < self.early_stop_threshold:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= 3:  # Wait for 3 epochs of no improvement\n",
    "                    print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            else:\n",
    "                no_improvement_count = 0\n",
    "            \n",
    "            prev_loss = avg_loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            z = sum(x[j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "            prob = self.sigmoid(z)\n",
    "            predictions.append(1 if prob >= 0.5 else 0)\n",
    "        return predictions\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics\n",
    "    Returns:\n",
    "        tuple: (accuracy, precision, recall, f1)\n",
    "    \"\"\"\n",
    "    tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n",
    "    tn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 0)\n",
    "    fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n",
    "    fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n",
    "    \n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def perform_cross_validation(X, y, lambda_values, k=5):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation to find the best regularization parameter\n",
    "    Args:\n",
    "        X (list): Feature vectors\n",
    "        y (list): Labels\n",
    "        lambda_values (list): List of lambda values to try\n",
    "        k (int): Number of folds\n",
    "    Returns:\n",
    "        float: Best lambda value\n",
    "    \"\"\"\n",
    "    print(\"\\nPerforming cross-validation...\")\n",
    "    n_samples = len(y)\n",
    "    fold_size = n_samples // k\n",
    "    best_lambda = None\n",
    "    best_score = -float('inf')\n",
    "    \n",
    "    for lambda_val in lambda_values:\n",
    "        print(f\"\\nTrying lambda = {lambda_val}\")\n",
    "        scores = []\n",
    "        \n",
    "        for fold in range(k):\n",
    "            # Split data into training and validation\n",
    "            val_start = fold * fold_size\n",
    "            val_end = val_start + fold_size\n",
    "            \n",
    "            X_val = X[val_start:val_end]\n",
    "            y_val = y[val_start:val_end]\n",
    "            X_train = X[:val_start] + X[val_end:]\n",
    "            y_train = y[:val_start] + y[val_end:]\n",
    "            \n",
    "            # Train model\n",
    "            model = LogisticRegression(lambda_reg=lambda_val, num_epochs=50)\n",
    "            model.train(X_train, y_train, method='minibatch')\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_val)\n",
    "            accuracy, _, _, f1 = evaluate(y_val, y_pred)\n",
    "            scores.append(f1)\n",
    "        \n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        print(f\"Average F1-score: {avg_score:.4f}\")\n",
    "        \n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_lambda = lambda_val\n",
    "    \n",
    "    print(f\"\\nBest lambda value: {best_lambda} (F1-score: {best_score:.4f})\")\n",
    "    return best_lambda\n",
    "\n",
    "def main():\n",
    "    # Example usage with a sample input file\n",
    "    # Assuming input file format: label\\ttext\n",
    "    file_path = \"SMSSpamCollection\"  # Replace with your data file path\n",
    "    \n",
    "    # Step 1: Load and split data\n",
    "    train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = load_and_split_data(file_path)\n",
    "    \n",
    "    # Step 2: Preprocess and tokenize texts\n",
    "    print(\"\\nPreprocessing and tokenizing texts...\")\n",
    "    train_tokens = [tokenize(preprocess_text(text)) for text in train_texts]\n",
    "    val_tokens = [tokenize(preprocess_text(text)) for text in val_texts]\n",
    "    test_tokens = [tokenize(preprocess_text(text)) for text in test_texts]\n",
    "    \n",
    "    # Step 3: Create TF-IDF features\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(train_tokens)\n",
    "    \n",
    "    X_train = vectorizer.transform(train_tokens)\n",
    "    X_val = vectorizer.transform(val_tokens)\n",
    "    X_test = vectorizer.transform(test_tokens)\n",
    "    \n",
    "    # Step 4: Train and evaluate with both SGD and mini-batch\n",
    "    for method in ['sgd', 'minibatch']:\n",
    "        print(f\"\\nStep 4: Training with {method.upper()}...\")\n",
    "        model = LogisticRegression(\n",
    "            learning_rate=0.1,  # Increased learning rate\n",
    "            lambda_reg=0.01,    # Reduced regularization\n",
    "            num_epochs=35,\n",
    "            early_stop_threshold=1e-4,\n",
    "            batch_size=32\n",
    "        )\n",
    "        model.train(X_train, train_labels, method=method)\n",
    "        \n",
    "        # Evaluate on both validation and test sets\n",
    "        print(f\"\\nStep 5: Evaluating {method.upper()} model...\")\n",
    "        \n",
    "        # Validation set evaluation\n",
    "        val_pred = model.predict(X_val)\n",
    "        val_accuracy, val_precision, val_recall, val_f1 = evaluate(val_labels, val_pred)\n",
    "        print(f\"\\n{method.upper()} Validation Results:\")\n",
    "        print(f\"Accuracy:  {val_accuracy:.4f}\")\n",
    "        print(f\"Precision: {val_precision:.4f}\")\n",
    "        print(f\"Recall:    {val_recall:.4f}\")\n",
    "        print(f\"F1-score:  {val_f1:.4f}\")\n",
    "        \n",
    "        # Test set evaluation\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_accuracy, test_precision, test_recall, test_f1 = evaluate(test_labels, test_pred)\n",
    "        print(f\"\\n{method.upper()} Test Results:\")\n",
    "        print(f\"Accuracy:  {test_accuracy:.4f}\")\n",
    "        print(f\"Precision: {test_precision:.4f}\")\n",
    "        print(f\"Recall:    {test_recall:.4f}\")\n",
    "        print(f\"F1-score:  {test_f1:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from SMSSpamCollection...\n",
      "Total number of samples loaded: 5574\n",
      "\n",
      "Label Distribution:\n",
      "ham: 4827 (86.60%)\n",
      "spam: 747 (13.40%)\n",
      "\n",
      "Preprocessing and tokenizing texts...\n",
      "\n",
      "Training with SGD...\n",
      "Class weights: {0: 0.5758783584292885, 1: 3.794747081712062}\n",
      "\n",
      "Training with SGD:\n",
      "Epoch\tLoss\t\tΔLoss\n",
      "0\t0.658678\tinf\n",
      "5\t0.485584\t0.010013\n",
      "10\t0.454077\t-0.003496\n",
      "15\t0.445167\t0.002118\n",
      "20\t0.443342\t0.000229\n",
      "25\t0.441141\t-0.000934\n",
      "30\t0.442649\t-0.001832\n",
      "\n",
      "Early stopping at epoch 31\n",
      "\n",
      "SGD Validation Results:\n",
      "Accuracy:  0.9510\n",
      "Precision: 0.7939\n",
      "Recall:    0.8814\n",
      "F1-score:  0.8353\n",
      "\n",
      "SGD Test Results:\n",
      "Accuracy:  0.9630\n",
      "Precision: 0.8333\n",
      "Recall:    0.9130\n",
      "F1-score:  0.8714\n",
      "\n",
      "Training with MINIBATCH...\n",
      "Class weights: {0: 0.5758783584292885, 1: 3.794747081712062}\n",
      "\n",
      "Training with MINIBATCH:\n",
      "Epoch\tLoss\t\tΔLoss\n",
      "0\t0.693433\tinf\n",
      "5\t0.687035\t0.001509\n",
      "10\t0.682705\t0.001094\n",
      "15\t0.678016\t0.001670\n",
      "20\t0.671605\t0.001584\n",
      "25\t0.667312\t0.001235\n",
      "30\t0.663232\t0.000150\n",
      "35\t0.658947\t0.000840\n",
      "40\t0.653376\t0.001105\n",
      "45\t0.649089\t-0.000153\n",
      "50\t0.642268\t0.001423\n",
      "55\t0.637316\t0.001314\n",
      "60\t0.633536\t0.001138\n",
      "65\t0.628988\t0.000384\n",
      "70\t0.624398\t0.001335\n",
      "75\t0.619958\t0.000983\n",
      "80\t0.616206\t0.000520\n",
      "85\t0.611941\t0.000375\n",
      "90\t0.607751\t0.001280\n",
      "95\t0.604215\t0.001137\n",
      "\n",
      "MINIBATCH Validation Results:\n",
      "Accuracy:  0.9498\n",
      "Precision: 0.8167\n",
      "Recall:    0.8305\n",
      "F1-score:  0.8235\n",
      "\n",
      "MINIBATCH Test Results:\n",
      "Accuracy:  0.9606\n",
      "Precision: 0.8417\n",
      "Recall:    0.8783\n",
      "F1-score:  0.8596\n",
      "Reading data from books.txt...\n",
      "Total number of samples loaded: 19536\n",
      "\n",
      "Label Distribution:\n",
      "Jane: 11054 (56.58%)\n",
      "Arthur: 2538 (12.99%)\n",
      "Fyodor: 5944 (30.43%)\n",
      "\n",
      "Preprocessing and tokenizing texts...\n",
      "\n",
      "Training multi-class logistic regression...\n",
      "\n",
      "Training with MINIBATCH:\n",
      "Epoch\tLoss\t\tΔLoss\n",
      "0\t1.122604\tinf\n",
      "5\t0.986854\t0.011026\n",
      "10\t0.938739\t0.009399\n",
      "15\t0.897059\t0.007880\n",
      "20\t0.860030\t0.007096\n",
      "25\t0.827634\t0.005915\n",
      "30\t0.798387\t0.005681\n",
      "35\t0.772917\t0.005007\n",
      "40\t0.750110\t0.004414\n",
      "45\t0.729940\t0.003875\n",
      "50\t0.712030\t0.003336\n",
      "55\t0.695988\t0.002865\n",
      "60\t0.681450\t0.002775\n",
      "65\t0.668306\t0.002468\n",
      "70\t0.656706\t0.002116\n",
      "75\t0.645997\t0.002099\n",
      "80\t0.636555\t0.001688\n",
      "85\t0.627737\t0.001668\n",
      "90\t0.619943\t0.001365\n",
      "95\t0.612556\t0.001496\n",
      "\n",
      "Evaluating model...\n",
      "\n",
      "Validation Results:\n",
      "Accuracy: 0.8802\n",
      "\n",
      "Arthur:\n",
      "Precision: 1.0000\n",
      "Recall:    0.4960\n",
      "F1-score:  0.6631\n",
      "\n",
      "Fyodor:\n",
      "Precision: 0.9960\n",
      "Recall:    0.8253\n",
      "F1-score:  0.9027\n",
      "\n",
      "Jane:\n",
      "Precision: 0.8245\n",
      "Recall:    1.0000\n",
      "F1-score:  0.9038\n",
      "\n",
      "Test Results:\n",
      "Accuracy: 0.8915\n",
      "\n",
      "Arthur:\n",
      "Precision: 1.0000\n",
      "Recall:    0.5409\n",
      "F1-score:  0.7021\n",
      "\n",
      "Fyodor:\n",
      "Precision: 0.9947\n",
      "Recall:    0.8396\n",
      "F1-score:  0.9106\n",
      "\n",
      "Jane:\n",
      "Precision: 0.8404\n",
      "Recall:    1.0000\n",
      "F1-score:  0.9133\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "def load_and_split_data(file_path, train_ratio=0.7, val_ratio=0.15, task='binary'):\n",
    "    \"\"\"Load and split data with hardcoded authors\"\"\"\n",
    "    print(f\"Reading data from {file_path}...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        if task == 'binary':\n",
    "            lines = [line.strip() for line in f if line.strip() and '\\t' in line]\n",
    "        else:\n",
    "            # Hardcoded authors\n",
    "            AUTHORS = [\n",
    "                'Jane Austen',\n",
    "                'Arthur Conan Doyle',\n",
    "                'Fyodor Dostoyevsky'\n",
    "            ]\n",
    "            \n",
    "            lines = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:  # Skip empty lines\n",
    "                    continue\n",
    "                \n",
    "                # Check each author\n",
    "                for author in AUTHORS:\n",
    "                    if line.startswith(author):\n",
    "                        text = line[len(author):].strip()\n",
    "                        if text:  # Ensure we have text after author name\n",
    "                            lines.append(f\"{author}|{text}\")\n",
    "                        break\n",
    "    \n",
    "    if not lines:\n",
    "        raise ValueError(f\"No valid data found in {file_path}\")\n",
    "    \n",
    "    print(f\"Total number of samples loaded: {len(lines)}\")\n",
    "    \n",
    "    # Count and display label distribution\n",
    "    if task == 'binary':\n",
    "        label_counts = Counter(line.split('\\t')[0] for line in lines)\n",
    "    else:\n",
    "        label_counts = Counter(line.split('|')[0] for line in lines)\n",
    "    \n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"{label}: {count} ({count/len(lines)*100:.2f}%)\")\n",
    "    \n",
    "    # Shuffle and split data\n",
    "    random.seed(42)\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    train_end = int(len(lines) * train_ratio)\n",
    "    val_end = int(len(lines) * (train_ratio + val_ratio))\n",
    "    \n",
    "    train_data = lines[:train_end]\n",
    "    val_data = lines[train_end:val_end]\n",
    "    test_data = lines[val_end:]\n",
    "    \n",
    "    def split_data(data, task):\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in data:\n",
    "            if task == 'binary':\n",
    "                if '\\t' in line:\n",
    "                    label, text = line.strip().split('\\t', 1)\n",
    "                    label_int = 1 if label.lower() == 'spam' else 0\n",
    "                    labels.append(label_int)\n",
    "                    sentences.append(text)\n",
    "            else:\n",
    "                if '|' in line:\n",
    "                    label, text = line.strip().split('|', 1)\n",
    "                    labels.append(label.strip())\n",
    "                    sentences.append(text.strip())\n",
    "        return sentences, labels\n",
    "    \n",
    "    return split_data(train_data, task) + split_data(val_data, task) + split_data(test_data, task)\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Enhanced text preprocessing\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = ' '.join(word for word in text.split() \n",
    "                   if not word.startswith(('http:', 'https:', 'www.')))\n",
    "    \n",
    "    # Keep only letters and spaces, replace other chars with space\n",
    "    cleaned_text = \"\"\n",
    "    for char in text:\n",
    "        if char.isalpha():\n",
    "            cleaned_text += char\n",
    "        elif char.isspace():\n",
    "            if not cleaned_text.endswith(' '):  # Avoid multiple spaces\n",
    "                cleaned_text += ' '\n",
    "    \n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Enhanced tokenization with expanded stopwords\"\"\"\n",
    "    stopwords = {\n",
    "        \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\", \n",
    "        \"has\", \"he\", \"in\", \"is\", \"it\", \"its\", \"of\", \"on\", \"that\", \"the\", \n",
    "        \"to\", \"was\", \"were\", \"will\", \"with\", \"the\", \"this\", \"but\", \"they\",\n",
    "        \"have\", \"had\", \"what\", \"when\", \"where\", \"who\", \"which\", \"why\", \"how\",\n",
    "        \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\",\n",
    "        \"such\", \"than\", \"too\", \"very\", \"can\", \"into\", \"if\", \"or\", \"i\", \"my\"\n",
    "    }\n",
    "    \n",
    "    def simple_stem(word):\n",
    "        \"\"\"Enhanced stemming with more rules\"\"\"\n",
    "        if len(word) < 4:\n",
    "            return word\n",
    "        \n",
    "        if word.endswith('ing'):\n",
    "            word = word[:-3]\n",
    "        elif word.endswith('ed'):\n",
    "            word = word[:-2]\n",
    "        elif word.endswith('ly'):\n",
    "            word = word[:-2]\n",
    "        elif word.endswith('s'):\n",
    "            word = word[:-1]\n",
    "        \n",
    "        return word\n",
    "    \n",
    "    words = text.split()\n",
    "    return [simple_stem(word) for word in words \n",
    "            if word not in stopwords and len(word) > 1]\n",
    "\n",
    "class TfidfVectorizer:\n",
    "    \"\"\"Improved TF-IDF vectorizer with better normalization\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocabulary = set()\n",
    "        self.idf = {}\n",
    "        self.vocab_index = {}\n",
    "        self.min_df = 2  # Minimum document frequency\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        # Count document frequencies\n",
    "        doc_freq = Counter()\n",
    "        for doc in documents:\n",
    "            doc_words = set(doc)  # Use set to count each word once per document\n",
    "            for word in doc_words:\n",
    "                doc_freq[word] += 1\n",
    "        \n",
    "        # Filter vocabulary based on minimum document frequency\n",
    "        self.vocabulary = {word for word, freq in doc_freq.items() \n",
    "                         if freq >= self.min_df}\n",
    "        \n",
    "        self.vocab_index = {word: idx for idx, word in enumerate(sorted(self.vocabulary))}\n",
    "        \n",
    "        # Calculate IDF scores with smoothing\n",
    "        num_docs = len(documents)\n",
    "        self.idf = {word: math.log((num_docs + 1)/(doc_freq[word] + 1)) + 1 \n",
    "                   for word in self.vocabulary}\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        X = []\n",
    "        for doc in documents:\n",
    "            tf = Counter(doc)\n",
    "            doc_len = len(doc) if len(doc) > 0 else 1\n",
    "            \n",
    "            features = [0.0] * len(self.vocabulary)\n",
    "            for word in set(doc):\n",
    "                if word in self.vocab_index:\n",
    "                    idx = self.vocab_index[word]\n",
    "                    tf_val = tf[word] / doc_len\n",
    "                    features[idx] = tf_val * self.idf.get(word, 0)\n",
    "            \n",
    "            # L2 normalize the feature vector\n",
    "            norm = math.sqrt(sum(x * x for x in features) + 1e-10)  # Add small epsilon\n",
    "            features = [x / norm for x in features]\n",
    "            \n",
    "            X.append(features)\n",
    "        return X\n",
    "class LogisticRegression:\n",
    "    \"\"\"Improved Logistic Regression with better numerical stability\"\"\"\n",
    "    def __init__(self, learning_rate=0.001, lambda_reg=0.001, num_epochs=100,\n",
    "                 early_stop_threshold=1e-5, batch_size=64, clip_value=5.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "        self.batch_size = batch_size\n",
    "        self.clip_value = clip_value\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "        self.training_history = []\n",
    "        self.momentum = 0.9\n",
    "        self.velocity_w = None\n",
    "        self.velocity_b = 0\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Numerically stable sigmoid\"\"\"\n",
    "        z = min(max(z, -100), 100)\n",
    "        return 1 / (1 + math.exp(-z))\n",
    "    \n",
    "    def clip_gradient(self, grad):\n",
    "        \"\"\"Gradient clipping to prevent explosion\"\"\"\n",
    "        norm = math.sqrt(sum(g * g for g in grad))\n",
    "        if norm > self.clip_value:\n",
    "            grad = [g * self.clip_value / norm for g in grad]\n",
    "        return grad\n",
    "    \n",
    "    def compute_loss(self, X, y, indices):\n",
    "        \"\"\"Compute binary cross-entropy loss with L2 regularization\"\"\"\n",
    "        loss = 0\n",
    "        for i in indices:\n",
    "            z = sum(X[i][j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "            pred = self.sigmoid(z)\n",
    "            # Add small epsilon for numerical stability\n",
    "            loss -= (y[i] * math.log(pred + 1e-15) + \n",
    "                    (1 - y[i]) * math.log(1 - pred + 1e-15))\n",
    "        \n",
    "        # L2 regularization\n",
    "        l2_term = self.lambda_reg * sum(w * w for w in self.weights)\n",
    "        return (loss / len(indices)) + l2_term\n",
    "    \n",
    "    def compute_class_weights(self, y):\n",
    "        \"\"\"Compute balanced class weights\"\"\"\n",
    "        counts = Counter(y)\n",
    "        total = len(y)\n",
    "        max_count = max(counts.values())\n",
    "        \n",
    "        weights = {\n",
    "            label: total / (len(counts) * count)\n",
    "            for label, count in counts.items()\n",
    "        }\n",
    "        print(f\"Class weights: {weights}\")\n",
    "        return weights\n",
    "    \n",
    "    def train(self, X, y, method='minibatch'):\n",
    "        \"\"\"Train with improved stability and momentum\"\"\"\n",
    "        if not self.weights:\n",
    "            n_features = len(X[0])\n",
    "            # Xavier initialization\n",
    "            scale = math.sqrt(2.0 / (n_features + 1))\n",
    "            self.weights = [random.uniform(-scale, scale) for _ in range(n_features)]\n",
    "            self.velocity_w = [0] * n_features\n",
    "        \n",
    "        class_weights = self.compute_class_weights(y)\n",
    "        print(f\"\\nTraining with {method.upper()}:\")\n",
    "        print(\"Epoch\\tLoss\\t\\tΔLoss\")\n",
    "        \n",
    "        n_samples = len(y)\n",
    "        prev_loss = float('inf')\n",
    "        best_loss = float('inf')\n",
    "        no_improvement_count = 0\n",
    "        best_weights = None\n",
    "        best_bias = None\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            indices = list(range(n_samples))\n",
    "            random.shuffle(indices)\n",
    "            \n",
    "            if method == 'sgd':\n",
    "                batch_indices = [[i] for i in indices]\n",
    "            else:\n",
    "                batch_indices = [indices[i:i + self.batch_size] \n",
    "                               for i in range(0, len(indices), self.batch_size)]\n",
    "            \n",
    "            if not batch_indices:  # Safety check\n",
    "                continue\n",
    "                \n",
    "            epoch_loss = 0\n",
    "            for batch in batch_indices:\n",
    "                weight_gradients = [0] * len(self.weights)\n",
    "                bias_gradient = 0\n",
    "                \n",
    "                for i in batch:\n",
    "                    z = sum(X[i][j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "                    pred = self.sigmoid(z)\n",
    "                    \n",
    "                    sample_weight = class_weights[y[i]]\n",
    "                    error = sample_weight * (pred - y[i])\n",
    "                    \n",
    "                    for j in range(len(self.weights)):\n",
    "                        weight_gradients[j] += error * X[i][j]\n",
    "                    bias_gradient += error\n",
    "                \n",
    "                # Clip gradients\n",
    "                weight_gradients = self.clip_gradient(weight_gradients)\n",
    "                bias_gradient = min(max(bias_gradient, -self.clip_value), self.clip_value)\n",
    "                \n",
    "                # Apply updates with momentum\n",
    "                batch_size = len(batch)\n",
    "                for j in range(len(self.weights)):\n",
    "                    reg_gradient = 2 * self.lambda_reg * self.weights[j]\n",
    "                    grad = weight_gradients[j]/batch_size + reg_gradient\n",
    "                    self.velocity_w[j] = (self.momentum * self.velocity_w[j] - \n",
    "                                        self.learning_rate * grad)\n",
    "                    self.weights[j] += self.velocity_w[j]\n",
    "                \n",
    "                self.velocity_b = (self.momentum * self.velocity_b - \n",
    "                                 self.learning_rate * (bias_gradient/batch_size))\n",
    "                self.bias += self.velocity_b\n",
    "                \n",
    "                epoch_loss += self.compute_loss(X, y, batch)\n",
    "            \n",
    "            avg_loss = epoch_loss / len(batch_indices)\n",
    "            loss_change = prev_loss - avg_loss\n",
    "            self.training_history.append(avg_loss)\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"{epoch}\\t{avg_loss:.6f}\\t{loss_change:.6f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                best_weights = self.weights.copy()\n",
    "                best_bias = self.bias\n",
    "                no_improvement_count = 0\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "            \n",
    "            # Early stopping with patience\n",
    "            if no_improvement_count >= 5:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "                self.weights = best_weights\n",
    "                self.bias = best_bias\n",
    "                break\n",
    "            \n",
    "            prev_loss = avg_loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            z = sum(x[j] * self.weights[j] for j in range(len(self.weights))) + self.bias\n",
    "            prob = self.sigmoid(z)\n",
    "            predictions.append(1 if prob >= 0.5 else 0)\n",
    "        return predictions\n",
    "    \n",
    "class MultiClassLogisticRegression:\n",
    "    \"\"\"Improved multi-class logistic regression with better stability\"\"\"\n",
    "    def __init__(self, n_classes, n_features, learning_rate=0.001, lambda_reg=0.001,\n",
    "                 num_epochs=100, early_stop_threshold=1e-5, batch_size=64):\n",
    "        self.n_classes = n_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = [[random.uniform(-0.1, 0.1) for _ in range(n_features)] \n",
    "                       for _ in range(n_classes)]\n",
    "        self.biases = [0] * n_classes\n",
    "        self.training_history = []\n",
    "        self.momentum = 0.9\n",
    "        self.velocity_w = [[0] * n_features for _ in range(n_classes)]\n",
    "        self.velocity_b = [0] * n_classes\n",
    "    \n",
    "    def softmax(self, scores):\n",
    "        \"\"\"Numerically stable softmax implementation\"\"\"\n",
    "        shifted_scores = [s - max(scores) for s in scores]  # Shift for numerical stability\n",
    "        exp_scores = [math.exp(s) for s in shifted_scores]\n",
    "        total = sum(exp_scores) + 1e-15  # Add small epsilon for stability\n",
    "        return [e / total for e in exp_scores]\n",
    "    \n",
    "    def compute_loss(self, X, y, indices):\n",
    "        \"\"\"Compute cross-entropy loss with L2 regularization\"\"\"\n",
    "        loss = 0\n",
    "        for i in indices:\n",
    "            # Compute scores for each class\n",
    "            scores = [sum(X[i][j] * self.weights[c][j] for j in range(len(X[i]))) + self.biases[c]\n",
    "                     for c in range(self.n_classes)]\n",
    "            probs = self.softmax(scores)\n",
    "            # Cross entropy loss\n",
    "            true_class = y[i]\n",
    "            loss -= math.log(probs[true_class] + 1e-15)\n",
    "        \n",
    "        # Add L2 regularization term\n",
    "        l2_term = self.lambda_reg * sum(sum(w * w for w in class_weights) \n",
    "                                      for class_weights in self.weights)\n",
    "        return (loss / len(indices)) + l2_term\n",
    "    \n",
    "    def train(self, X, y, method='minibatch'):\n",
    "        \"\"\"Train with improved stability and momentum\"\"\"\n",
    "        n_samples = len(y)\n",
    "        print(f\"\\nTraining with {method.upper()}:\")\n",
    "        print(\"Epoch\\tLoss\\t\\tΔLoss\")\n",
    "        \n",
    "        prev_loss = float('inf')\n",
    "        best_loss = float('inf')\n",
    "        no_improvement_count = 0\n",
    "        best_weights = None\n",
    "        best_biases = None\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            indices = list(range(n_samples))\n",
    "            random.shuffle(indices)\n",
    "            \n",
    "            batch_indices = [indices[i:i + self.batch_size] \n",
    "                           for i in range(0, len(indices), self.batch_size)]\n",
    "            \n",
    "            if not batch_indices:  # Safety check\n",
    "                continue\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            for batch in batch_indices:\n",
    "                # Initialize gradients\n",
    "                weight_gradients = [[0] * len(self.weights[0]) for _ in range(self.n_classes)]\n",
    "                bias_gradients = [0] * self.n_classes\n",
    "                \n",
    "                for i in batch:\n",
    "                    # Forward pass\n",
    "                    scores = [sum(X[i][j] * self.weights[c][j] for j in range(len(X[i]))) + self.biases[c]\n",
    "                            for c in range(self.n_classes)]\n",
    "                    probs = self.softmax(scores)\n",
    "                    \n",
    "                    # Compute gradients\n",
    "                    true_class = y[i]\n",
    "                    for c in range(self.n_classes):\n",
    "                        error = probs[c]\n",
    "                        if c == true_class:\n",
    "                            error -= 1\n",
    "                        \n",
    "                        # Accumulate gradients\n",
    "                        for j in range(len(X[i])):\n",
    "                            weight_gradients[c][j] += error * X[i][j]\n",
    "                        bias_gradients[c] += error\n",
    "                \n",
    "                # Apply updates with momentum\n",
    "                batch_size = len(batch)\n",
    "                for c in range(self.n_classes):\n",
    "                    for j in range(len(self.weights[c])):\n",
    "                        reg_gradient = 2 * self.lambda_reg * self.weights[c][j]\n",
    "                        grad = weight_gradients[c][j]/batch_size + reg_gradient\n",
    "                        self.velocity_w[c][j] = (self.momentum * self.velocity_w[c][j] - \n",
    "                                               self.learning_rate * grad)\n",
    "                        self.weights[c][j] += self.velocity_w[c][j]\n",
    "                    \n",
    "                    self.velocity_b[c] = (self.momentum * self.velocity_b[c] - \n",
    "                                        self.learning_rate * (bias_gradients[c]/batch_size))\n",
    "                    self.biases[c] += self.velocity_b[c]\n",
    "                \n",
    "                batch_loss = self.compute_loss(X, y, batch)\n",
    "                if not math.isnan(batch_loss):  # Skip invalid losses\n",
    "                    epoch_loss += batch_loss\n",
    "            \n",
    "            if batch_indices:  # Ensure we don't divide by zero\n",
    "                avg_loss = epoch_loss / len(batch_indices)\n",
    "                loss_change = prev_loss - avg_loss\n",
    "                self.training_history.append(avg_loss)\n",
    "                \n",
    "                if epoch % 5 == 0:\n",
    "                    print(f\"{epoch}\\t{avg_loss:.6f}\\t{loss_change:.6f}\")\n",
    "                \n",
    "                # Save best model\n",
    "                if avg_loss < best_loss:\n",
    "                    best_loss = avg_loss\n",
    "                    best_weights = [w[:] for w in self.weights]\n",
    "                    best_biases = self.biases[:]\n",
    "                    no_improvement_count = 0\n",
    "                else:\n",
    "                    no_improvement_count += 1\n",
    "                \n",
    "                # Early stopping with patience\n",
    "                if no_improvement_count >= 5:\n",
    "                    print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "                    self.weights = best_weights\n",
    "                    self.biases = best_biases\n",
    "                    break\n",
    "                \n",
    "                prev_loss = avg_loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            scores = [sum(x[j] * self.weights[c][j] for j in range(len(x))) + self.biases[c]\n",
    "                     for c in range(self.n_classes)]\n",
    "            predictions.append(scores.index(max(scores)))\n",
    "        return predictions\n",
    "\n",
    "def evaluate_multiclass(y_true, y_pred, classes):\n",
    "    \"\"\"Compute evaluation metrics for multi-class classification\"\"\"\n",
    "    if not y_true or not y_pred:\n",
    "        raise ValueError(\"Empty prediction or ground truth arrays\")\n",
    "    \n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(f\"Length mismatch: y_true ({len(y_true)}) != y_pred ({len(y_pred)})\")\n",
    "    \n",
    "    n_classes = len(classes)\n",
    "    if n_classes < 2:\n",
    "        raise ValueError(f\"Need at least 2 classes, got {n_classes}\")\n",
    "    \n",
    "    confusion = [[0] * n_classes for _ in range(n_classes)]\n",
    "    \n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if not (0 <= yt < n_classes) or not (0 <= yp < n_classes):\n",
    "            raise ValueError(f\"Invalid class indices: true={yt}, pred={yp}\")\n",
    "        confusion[yt][yp] += 1\n",
    "    \n",
    "    total_samples = len(y_true)\n",
    "    if total_samples == 0:\n",
    "        raise ValueError(\"No samples to evaluate\")\n",
    "        \n",
    "    accuracy = sum(confusion[i][i] for i in range(n_classes)) / total_samples\n",
    "    metrics = {'accuracy': accuracy, 'per_class': {}}\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        tp = confusion[i][i]\n",
    "        fp = sum(confusion[j][i] for j in range(n_classes)) - tp\n",
    "        fn = sum(confusion[i][j] for j in range(n_classes)) - tp\n",
    "        \n",
    "        # Add small epsilon to prevent division by zero\n",
    "        epsilon = 1e-10\n",
    "        precision = tp / (tp + fp + epsilon)\n",
    "        recall = tp / (tp + fn + epsilon)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "        \n",
    "        metrics['per_class'][class_name] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def main():\n",
    "    # Binary classification (SMS spam)\n",
    "    file_path = \"SMSSpamCollection\"\n",
    "    train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = load_and_split_data(file_path)\n",
    "    \n",
    "    print(\"\\nPreprocessing and tokenizing texts...\")\n",
    "    train_tokens = [tokenize(preprocess_text(text)) for text in train_texts]\n",
    "    val_tokens = [tokenize(preprocess_text(text)) for text in val_texts]\n",
    "    test_tokens = [tokenize(preprocess_text(text)) for text in test_texts]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(train_tokens)\n",
    "    \n",
    "    X_train = vectorizer.transform(train_tokens)\n",
    "    X_val = vectorizer.transform(val_tokens)\n",
    "    X_test = vectorizer.transform(test_tokens)\n",
    "    \n",
    "    for method in ['sgd', 'minibatch']:\n",
    "        print(f\"\\nTraining with {method.upper()}...\")\n",
    "        model = LogisticRegression(\n",
    "            learning_rate=0.001,\n",
    "            lambda_reg=0.001,\n",
    "            num_epochs=100,\n",
    "            early_stop_threshold=1e-5,\n",
    "            batch_size=64\n",
    "        )\n",
    "        model.train(X_train, train_labels, method=method)\n",
    "        \n",
    "        # Validation set evaluation\n",
    "        val_pred = model.predict(X_val)\n",
    "        val_accuracy, val_precision, val_recall, val_f1 = evaluate(val_labels, val_pred)\n",
    "        print(f\"\\n{method.upper()} Validation Results:\")\n",
    "        print(f\"Accuracy:  {val_accuracy:.4f}\")\n",
    "        print(f\"Precision: {val_precision:.4f}\")\n",
    "        print(f\"Recall:    {val_recall:.4f}\")\n",
    "        print(f\"F1-score:  {val_f1:.4f}\")\n",
    "        \n",
    "        # Test set evaluation\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_accuracy, test_precision, test_recall, test_f1 = evaluate(test_labels, test_pred)\n",
    "        print(f\"\\n{method.upper()} Test Results:\")\n",
    "        print(f\"Accuracy:  {test_accuracy:.4f}\")\n",
    "        print(f\"Precision: {test_precision:.4f}\")\n",
    "        print(f\"Recall:    {test_recall:.4f}\")\n",
    "        print(f\"F1-score:  {test_f1:.4f}\")\n",
    "\n",
    "def main_multiclass():\n",
    "    \"\"\"Main function for multi-class author classification\"\"\"\n",
    "    file_path = \"books.txt\"\n",
    "    \n",
    "    # Step 1: Load and split data\n",
    "    train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = \\\n",
    "        load_and_split_data(file_path, task='multiclass')\n",
    "    \n",
    "    # Get unique classes and create label mapping\n",
    "    classes = sorted(set(train_labels))\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(classes)}\n",
    "    \n",
    "    # Convert string labels to indices\n",
    "    train_labels = [label_to_idx[label] for label in train_labels]\n",
    "    val_labels = [label_to_idx[label] for label in val_labels]\n",
    "    test_labels = [label_to_idx[label] for label in test_labels]\n",
    "    \n",
    "    # Step 2: Preprocess and tokenize texts\n",
    "    print(\"\\nPreprocessing and tokenizing texts...\")\n",
    "    train_tokens = [tokenize(preprocess_text(text)) for text in train_texts]\n",
    "    val_tokens = [tokenize(preprocess_text(text)) for text in val_texts]\n",
    "    test_tokens = [tokenize(preprocess_text(text)) for text in test_texts]\n",
    "    \n",
    "    # Step 3: Create TF-IDF features\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(train_tokens)\n",
    "    \n",
    "    X_train = vectorizer.transform(train_tokens)\n",
    "    X_val = vectorizer.transform(val_tokens)\n",
    "    X_test = vectorizer.transform(test_tokens)\n",
    "    \n",
    "    # Step 4: Train model\n",
    "    print(\"\\nTraining multi-class logistic regression...\")\n",
    "    model = MultiClassLogisticRegression(\n",
    "        n_classes=len(classes),\n",
    "        n_features=len(vectorizer.vocabulary),\n",
    "        learning_rate=0.001,\n",
    "        lambda_reg=0.001,\n",
    "        num_epochs=100,\n",
    "        batch_size=64\n",
    "    )\n",
    "    \n",
    "    model.train(X_train, train_labels, method='minibatch')\n",
    "    \n",
    "    # Step 5: Evaluate\n",
    "    print(\"\\nEvaluating model...\")\n",
    "    \n",
    "    # Validation set evaluation\n",
    "    val_pred = model.predict(X_val)\n",
    "    val_metrics = evaluate_multiclass(val_labels, val_pred, classes)\n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    for class_name, metrics in val_metrics['per_class'].items():\n",
    "        print(f\"\\n{class_name}:\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"F1-score:  {metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Test set evaluation\n",
    "    test_pred = model.predict(X_test)\n",
    "    test_metrics = evaluate_multiclass(test_labels, test_pred, classes)\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    for class_name, metrics in test_metrics['per_class'].items():\n",
    "        print(f\"\\n{class_name}:\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"F1-score:  {metrics['f1']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run binary classification\n",
    "    main()\n",
    "    # Run multi-class classification\n",
    "    main_multiclass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
